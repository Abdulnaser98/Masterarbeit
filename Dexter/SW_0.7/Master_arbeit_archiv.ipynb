{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4ca355",
   "metadata": {},
   "source": [
    "# Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f12c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03db4bc",
   "metadata": {},
   "source": [
    "# Set the pathes to the data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc3ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_working_dir = '/Users/abdulnaser/Desktop/Masterarbeit/metadatatransferlearning-main/meta_tl/'\n",
    "path_to_data_folder = path_to_working_dir + 'data/'\n",
    "path_to_sim_vector_folder =  path_to_data_folder + 'sim_dataframes/'\n",
    "models_directory = path_to_working_dir + 'models/'\n",
    "path_to_results_folder = path_to_data_folder + 'results/'\n",
    "path_to_feature_vectors_folder = path_to_data_folder + 'feature_vectors/'\n",
    "active_learning_path = path_to_data_folder + 'active_learning/'\n",
    "path_to_models_folders = path_to_data_folder + 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a6ae1",
   "metadata": {},
   "source": [
    "# Initial Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd4ed6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_config = {\n",
    "    'data_duplicated': True, # Set to True if data duplication is enabled , False otherwise\n",
    "    'meta_data_representation': 1, # Options: 1:generate_data_representation_initial_features(), 2:generate_data_representation_better_values,3:generate_data_representations_combined_features,4:generate_data_representations_mean_embeddings \n",
    "    'training_instance_based': True # Set to True for instance-based training, False otherwistraining_instance_based\n",
    "} \n",
    "\n",
    "meta_data_rep_mapping = {\n",
    "    1:'generate_data_representation_initial_features',\n",
    "    2:'generate_data_representation_better_features',\n",
    "    3:'generate_data_representations_combined_features',\n",
    "    4:'generate_data_representations_mean_embeddings'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec86258",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a0fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Euclidean distance between rows\n",
    "def calculate_distances(row,df, columns_to_consider):        \n",
    "    distances = euclidean_distances([row[columns_to_consider]],df[columns_to_consider])\n",
    "    return distances.flatten()\n",
    "   \n",
    "\n",
    "# Extract the closest 7 rows and calculate the ratio of 'is_match' values\n",
    "def calcualte_ratio(row, df,columns_to_consider):\n",
    "    #print(\"The following row has the value of 'is_match' {}\".format(row['is_match']))\n",
    "    distances = calculate_distances(row,df,columns_to_consider)\n",
    "    closest_indices = np.argsort(distances)[1:6] # Exclude the row itself\n",
    "    closest_rows = df.iloc[closest_indices]\n",
    "    # Count the occurrences of the same 'is_match' label\n",
    "    match_count = (closest_rows['is_match'] == row['is_match']).sum()\n",
    "    \n",
    "    # Calculate the ratio\n",
    "    ratio = match_count / len(closest_rows)\n",
    "                \n",
    "    return ratio\n",
    "\n",
    "\n",
    "def del_file(file_path):\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file} deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {file_path}: {e}\")\n",
    "    \n",
    "\n",
    "\n",
    "def delete_files_from_folder(folder_path,files_to_del='ALL'):  \n",
    "    if files_to_del == 'ALL':\n",
    "        # List all files in the folder\n",
    "        files = os.listdir(folder_path)\n",
    "\n",
    "        # Iterate through the files and delete them\n",
    "        for file in files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            del_file(file_path)\n",
    "    else:\n",
    "        full_file_path = os.path.join(folder_path, files_to_del)\n",
    "        del_file(full_file_path)\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726efd7a",
   "metadata": {},
   "source": [
    "# The attributes of the feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f1db75",
   "metadata": {},
   "source": [
    "feature vectors to extract:\n",
    "    \n",
    "1. Number of columns that have at least one value \n",
    "\n",
    "2. \"Modell_no_list_truncatebegin20\" (Prozent der non-Null Werte,Min,Mean,Max, Median)\n",
    "\n",
    "3. \"MPN_Liste_TruncateBegin20\"  (prozent der non-Null Werte,Min,Mean,Max,Median)\n",
    "\n",
    "4. \"EAN_Liste_TruncateBegin20\" (Pozent der non-Null Werte,Min,Mean,Max,Median)\n",
    "\n",
    "5. \"Produktname_dic3\" (Prozent der non-Null Werte,Min,Mean,Max,median)\n",
    "\n",
    "6. \"Modell_Liste_3g\" (Prozent der non-Null Werte,Min,Mean,Max,Median)\n",
    "\n",
    "7. \"Digital_zoom_NumMaxProz30\" (Prozent der non-Null Werte,Min,Mean,Max,Median)\n",
    "\n",
    "8. \"optischer_zoom_NumMaxProz30\" (Prozent der non-Null Werte,Min,Mean,Max,Median)\n",
    "\n",
    "9. \"Breite_NumMaxProz30\" (Prozent der non-Null Werte,Min,Mean,Max,Median)\n",
    "\n",
    "10. \"HÃ¶he_NumMaxProz30\" (Prozent der non-Null Werte,Min,Mean,Max,Median)\n",
    "\n",
    "11. \"Gewicht_NumMaxProz30\" (Prozent der non-Null Werte,Min,Mean,Max,Median)\n",
    "\n",
    "12. \"Sensortyp_Jaccard3\" (Prozent der non-Null Werte,Min,Mean,Max,Median)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695d6d92",
   "metadata": {},
   "source": [
    "# Generate feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b2ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_similairty_value(list1, list2):\n",
    "    \"\"\"\n",
    "    Calculate normalized inverse Euclidean similarity between two lists.\n",
    "\n",
    "    Parameters:\n",
    "    - list1: First list\n",
    "    - list2: Second list\n",
    "\n",
    "    Returns:\n",
    "    - Normalized inverse Euclidean similarity (a float between 0 and 1)\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "\n",
    "    # Ensure the lists have the same length\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Lists must have the same length\")\n",
    "\n",
    "    # Calculate Euclidean distance for each pair of values\n",
    "    for value1, value2 in zip(list1, list2):\n",
    "        distance = math.sqrt((value1 - value2) ** 2)\n",
    "        distances.append(distance)\n",
    "\n",
    "    # Check if all distances are zero (identical lists)\n",
    "    if all(distance == 0 for distance in distances):\n",
    "        return 1.0  # Identical lists, return maximum similarity\n",
    "\n",
    "    # Normalize the distances and calculate the average\n",
    "    max_distance = max(distances)\n",
    "    normalized_distances = [1 - (distance / max_distance) for distance in distances]\n",
    "    similarity_score = sum(normalized_distances) / len(normalized_distances)\n",
    "\n",
    "    return similarity_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb79d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_similarity(list1, list2):\n",
    "    # Check if the lists have the same length\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Both lists must have the same length\")\n",
    "\n",
    "    # Calculate absolute differences\n",
    "    abs_diff = [abs(a - b) for a, b in zip(list1, list2)]\n",
    "\n",
    "    # Normalize and calculate the mean of absolute differences\n",
    "    mean_normalized_diff = 1.0 - (sum(abs_diff) / len(list1))\n",
    "\n",
    "    return mean_normalized_diff\n",
    "\n",
    "\n",
    "similarity_score = compute_similarity(values1, values2)\n",
    "print(f\"Similarity Score: {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_representation_very_new_test():\n",
    "    \n",
    "    # Get a list of all CSV files in the folder \n",
    "    csv_files = [file for file in os.listdir(path_to_sim_vector_folder) if file.endswith('.csv')]\n",
    "\n",
    "\n",
    "    # Create an empty list to store feature vectors\n",
    "    feature_vectors_list = []\n",
    "\n",
    "    # Value to exclude from feature extraction\n",
    "    nan_replacement_value = 2\n",
    "\n",
    "    # Columns to extracte from feature extraction \n",
    "    to_consider_columns = ['Produktname_dic3','Modell_Liste_3g']\n",
    "\n",
    "\n",
    "    # Iterate through each CSV file and generate statistics\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(path_to_sim_vector_folder, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df[to_consider_columns]\n",
    "        # convert \"/\" into Nan\n",
    "        df = df.apply(pd.to_numeric, errors= 'coerce')\n",
    "\n",
    "\n",
    "        # Calculate statistics\n",
    "        column_std_deviation = df.std().round(2)\n",
    "        column_mean = df.mean().round(2)\n",
    "       \n",
    "        \n",
    "    \n",
    "    \n",
    "        # Create a Series for the names of the sim-vectors\n",
    "        file_name_series = pd.Series([csv_file], index=['file_name'])\n",
    "\n",
    "        # Create a Series for non_nan_columns_count\n",
    "        #non_nan_columns_count_series = pd.Series([non_nan_columns_count], index=['non_nan_columns_count'])\n",
    "\n",
    "        # Concatenate statistics into a feature vector\n",
    "        feature_vector = pd.concat([file_name_series,column_mean,column_std_deviation], axis=0)\n",
    "\n",
    "        # Replace NaN values in the feature vector with a specific numerical value\n",
    "        feature_vector.fillna(nan_replacement_value, inplace=True)\n",
    "\n",
    "        # Convert the feature vector to a list and append it to the feature vectors list\n",
    "        feature_vectors_list.append(feature_vector.tolist())\n",
    "    \n",
    "    # Convert the list of lists to a DataFrame\n",
    "    df = pd.DataFrame(feature_vectors_list)\n",
    "    # Assign names to columns\n",
    "    column_names = [f\"feature_{i+1}\" for i in range(df.shape[1])]\n",
    "    df.columns = column_names\n",
    "\n",
    "    return df,df['feature_1']\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9de234",
   "metadata": {},
   "outputs": [],
   "source": [
    "d , s = generate_data_representation_very_new_test()\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'Age' column (descending order)\n",
    "df_sorted_desc = d.sort_values(by=['feature_4','feature_5','feature_2','feature_3'], ascending=False)\n",
    "df_sorted_desc.to_csv('testnew.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6009bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "d,d_1= generate_data_representation_very_new_test()\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47578a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "d,t = generate_data_representation_very_new_test()\n",
    "d.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1758455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_representation_very_new():\n",
    "    \n",
    "    # Checks if the feature vector file already exists and if it exits , then remove it\n",
    "    delete_files_from_folder(path_to_feature_vectors_folder,'feature_vectors_init_features.csv')\n",
    "     \n",
    "    # Get a list of all CSV files in the folder \n",
    "    csv_files = [file for file in os.listdir(path_to_sim_vector_folder) if file.endswith('.csv')]\n",
    "\n",
    "\n",
    "    # Create an empty list to store feature vectors\n",
    "    feature_vectors_list = []\n",
    "\n",
    "    # Value to exclude from feature extraction\n",
    "    nan_replacement_value = 2 \n",
    "\n",
    "    # Columns to extracte from feature extraction \n",
    "    to_consider_columns = ['Produktname_dic3','Modell_Liste_3g']\n",
    "\n",
    "\n",
    "    # Iterate through each CSV file and generate statistics\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(path_to_sim_vector_folder, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df[to_consider_columns]\n",
    "        # convert \"/\" into Nan\n",
    "        df = df.apply(pd.to_numeric, errors= 'coerce')\n",
    "\n",
    "\n",
    "        # Calculate statistics\n",
    "        # The number of columns that have at least one non-Nan Value\n",
    "        non_nan_columns_count = np.sum(df.notnull().any())\n",
    "        # The percentage of the Nan values in each column \n",
    "        nan_percentage = ((df.isnull().sum() / len(df)) * 100).round(2)\n",
    "        column_min = df.min().round(2)\n",
    "        column_mean = df.mean().round(2)\n",
    "        column_max = df.max().round(2)\n",
    "        column_25th_percentile = df.quantile(0.25).round(2)\n",
    "        column_median = df.quantile(0.5).round(2)\n",
    "        column_75th_percentile = df.quantile(0.75).round(2)\n",
    "        \n",
    "    \n",
    "    \n",
    "        # Create a Series for the names of the sim-vectors\n",
    "        file_name_series = pd.Series([csv_file], index=['file_name'])\n",
    "\n",
    "        # Create a Series for non_nan_columns_count\n",
    "        non_nan_columns_count_series = pd.Series([non_nan_columns_count], index=['non_nan_columns_count'])\n",
    "\n",
    "        # Concatenate statistics into a feature vector\n",
    "        feature_vector = pd.concat([file_name_series,non_nan_columns_count_series,column_min, column_mean, column_max,column_25th_percentile,column_median,column_75th_percentile], axis=0)\n",
    "\n",
    "        # Replace NaN values in the feature vector with a specific numerical value\n",
    "        feature_vector.fillna(nan_replacement_value, inplace=True)\n",
    "\n",
    "        # Convert the feature vector to a list and append it to the feature vectors list\n",
    "        feature_vectors_list.append(feature_vector.tolist())\n",
    "    \n",
    "    # Convert the list of lists to a DataFrame\n",
    "    df = pd.DataFrame(feature_vectors_list, columns = ['compared_resources', 'Numer_of_columns_with_at_least_one_values', \n",
    "                                                       'Produktname_dic3_min', 'Modell_Liste_min','Produktname_dic3_mean',\n",
    "                                                       'Modell_Liste_mean','Produktname_dic3_max', 'Modell_Liste_max','Produktname_dic3_25th_percentile', \n",
    "                                                       'Modell_Liste_25th_percentile','Produktname_median', 'Modell_Liste_median', 'Produktname_75th_percentile',\n",
    "                                                       'Modell_Liste_75th_percentile'])\n",
    "                                                       \n",
    "    # Order the columns in the dataframe according to the following order\n",
    "    columns_to_order = ['compared_resources', 'Numer_of_columns_with_at_least_one_values', \n",
    "                        'Produktname_dic3_min', 'Produktname_dic3_mean', 'Produktname_dic3_max','Produktname_dic3_25th_percentile','Produktname_median','Produktname_75th_percentile',\n",
    "                        'Modell_Liste_min', 'Modell_Liste_mean', 'Modell_Liste_max','Modell_Liste_25th_percentile','Modell_Liste_median','Modell_Liste_75th_percentile'                                \n",
    "                        ]\n",
    "\n",
    "    df = df[columns_to_order]\n",
    "    df.to_csv(os.path.join(path_to_feature_vectors_folder,'feature_vectors_init_features.csv'))\n",
    "\n",
    "    return df,df['compared_resources']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b057728",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_1 = generate_data_representation_very_new()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dce32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_representation_initial_features():\n",
    "    \n",
    "    # Checks if the feature vector file already exists and if it exits , then remove it\n",
    "    delete_files_from_folder(path_to_feature_vectors_folder,'feature_vectors_init_features.csv')\n",
    "     \n",
    "    # Get a list of all CSV files in the folder \n",
    "    csv_files = [file for file in os.listdir(path_to_sim_vector_folder) if file.endswith('.csv')]\n",
    "\n",
    "\n",
    "    # Create an empty list to store feature vectors\n",
    "    feature_vectors_list = []\n",
    "\n",
    "    # Value to exclude from feature extraction\n",
    "    nan_replacement_value = 2 \n",
    "\n",
    "    # Columns to extracte from feature extraction \n",
    "    to_consider_columns = ['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "                           'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30', 'Breite_NumMaxProz30', \n",
    "                           'HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3']\n",
    "\n",
    "\n",
    "    # Iterate through each CSV file and generate statistics\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(path_to_sim_vector_folder, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df[to_consider_columns]\n",
    "        # convert \"/\" into Nan\n",
    "        df = df.apply(pd.to_numeric, errors= 'coerce')\n",
    "\n",
    "\n",
    "        # Calculate statistics\n",
    "        # The number of columns that have at least one non-Nan Value\n",
    "        non_nan_columns_count = np.sum(df.notnull().any())\n",
    "        # The percentage of the Nan values in each column \n",
    "        nan_percentage = ((df.isnull().sum() / len(df)) * 100).round(2)\n",
    "        column_min = df.min().round(2)\n",
    "        column_mean = df.mean().round(2)\n",
    "        column_max = df.max().round(2)\n",
    "        column_median = df.median().round(2)\n",
    "    \n",
    "    \n",
    "        # Create a Series for the names of the sim-vectors\n",
    "        file_name_series = pd.Series([csv_file], index=['file_name'])\n",
    "\n",
    "        # Create a Series for non_nan_columns_count\n",
    "        non_nan_columns_count_series = pd.Series([non_nan_columns_count], index=['non_nan_columns_count'])\n",
    "\n",
    "        # Concatenate statistics into a feature vector\n",
    "        feature_vector = pd.concat([file_name_series,non_nan_columns_count_series,nan_percentage,column_min, column_mean, column_max, column_median], axis=0)\n",
    "\n",
    "        # Replace NaN values in the feature vector with a specific numerical value\n",
    "        feature_vector.fillna(nan_replacement_value, inplace=True)\n",
    "\n",
    "        # Convert the feature vector to a list and append it to the feature vectors list\n",
    "        feature_vectors_list.append(feature_vector.tolist())\n",
    "    \n",
    "    # Convert the list of lists to a DataFrame\n",
    "    df = pd.DataFrame(feature_vectors_list, columns = ['compared_resources', 'Numer_of_columns_with_at_least_one_values', \n",
    "                                                       'MPN_Liste_perc_nan', 'EAN_Liste_perc_nan', 'Produktname_dic3_perc_nan',\n",
    "                                                       'Modell_Liste_perc_nan', 'Digital_zoom_perc_nan', 'optischer_zoom_perc_nan',\n",
    "                                                       'Breite_perc_nan', 'HÃ¶he_perc_nan', 'Gewicht_perc_nan','Sensortyp_perc_nan',\n",
    "                                                       'MPN_Liste_min', 'EAN_Liste_min', 'Produktname_dic3_min', 'Modell_Liste_min', \n",
    "                                                       'Digital_zoom_min', 'optischer_zoom_min', 'Breite_min', 'HÃ¶he_min',\n",
    "                                                       'Gewicht_min', 'Sensortyp_min', 'MPN_Liste_mean', 'EAN_Liste_mean', 'Produktname_dic3_mean',\n",
    "                                                       'Modell_Liste_mean', 'Digital_zoom_mean','optischer_zoom_mean', 'Breite_mean',\n",
    "                                                       'HÃ¶he_mean','Gewicht_mean','Sensortyp_mean', 'MPN_Liste_max', 'EAN_Liste_max',\n",
    "                                                       'Produktname_dic3_max', 'Modell_Liste_max', 'Digital_zoom_max', 'optischer_zoom_max',\n",
    "                                                       'Breite_max', 'HÃ¶he_max', 'Gewicht_max', 'Sensortyp_max',\n",
    "                                                       'MPN_Liste_median', 'EAN_Liste_median', 'Produktname_dic3_median', 'Modell_Liste_median',\n",
    "                                                       'Digital_zoom_median', 'optischer_zoom_median','Breite_median','HÃ¶he_median','Gewicht_median','Sensortyp_median'])\n",
    "\n",
    "\n",
    "    # Order the columns in the dataframe according to the following order\n",
    "    columns_to_order = ['compared_resources', 'Numer_of_columns_with_at_least_one_values', \n",
    "                        'MPN_Liste_perc_nan', 'MPN_Liste_min', 'MPN_Liste_mean', 'MPN_Liste_max','MPN_Liste_median',\n",
    "                        'EAN_Liste_perc_nan', 'EAN_Liste_min', 'EAN_Liste_mean', 'EAN_Liste_max','EAN_Liste_median',\n",
    "                        'Produktname_dic3_perc_nan', 'Produktname_dic3_min', 'Produktname_dic3_mean', 'Produktname_dic3_max', 'Produktname_dic3_median',\n",
    "                        'Modell_Liste_perc_nan', 'Modell_Liste_min', 'Modell_Liste_mean', 'Modell_Liste_max', 'Modell_Liste_median',\n",
    "                        'Digital_zoom_perc_nan', 'Digital_zoom_min', 'Digital_zoom_mean', 'Digital_zoom_max', 'Digital_zoom_median',\n",
    "                        'optischer_zoom_perc_nan', 'optischer_zoom_min', 'optischer_zoom_mean', 'optischer_zoom_max','optischer_zoom_median',\n",
    "                        'Breite_perc_nan', 'Breite_min', 'Breite_mean', 'Breite_max', 'Breite_median',\n",
    "                        'HÃ¶he_perc_nan', 'HÃ¶he_min', 'HÃ¶he_mean', 'HÃ¶he_max', 'HÃ¶he_median',\n",
    "                        'Gewicht_perc_nan', 'Gewicht_min', 'Gewicht_mean', 'Gewicht_max', 'Gewicht_median',\n",
    "                        'Sensortyp_perc_nan', 'Sensortyp_min', 'Sensortyp_mean', 'Sensortyp_max', 'Sensortyp_median'                                             \n",
    "                        ]\n",
    "\n",
    "    df = df[columns_to_order]\n",
    "    df.to_csv(os.path.join(path_to_feature_vectors_folder,'feature_vectors_init_features.csv'))\n",
    "\n",
    "    return df,df['compared_resources']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d65d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_representation_initial_few_features():\n",
    "    \n",
    "    # Checks if the feature vector file already exists and if it exits , then remove it\n",
    "    #delete_files_from_folder(path_to_feature_vectors_folder,'feature_vectors_init_features.csv')\n",
    "     \n",
    "    # Get a list of all CSV files in the folder \n",
    "    csv_files = [file for file in os.listdir(path_to_sim_vector_folder) if file.endswith('.csv')]\n",
    "\n",
    "\n",
    "    # Create an empty list to store feature vectors\n",
    "    feature_vectors_list = []\n",
    "\n",
    "    # Value to exclude from feature extraction\n",
    "    nan_replacement_value = 2 \n",
    "\n",
    "    # Columns to extracte from feature extraction \n",
    "    to_consider_columns = ['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3']\n",
    "\n",
    "\n",
    "    # Iterate through each CSV file and generate statistics\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(path_to_sim_vector_folder, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df[to_consider_columns]\n",
    "        # convert \"/\" into Nan\n",
    "        df = df.apply(pd.to_numeric, errors= 'coerce')\n",
    "\n",
    "\n",
    "        # Calculate statistics\n",
    "        # The number of columns that have at least one non-Nan Value\n",
    "        non_nan_columns_count = np.sum(df.notnull().any())\n",
    "        # The percentage of the Nan values in each column \n",
    "        nan_percentage = ((df.isnull().sum() / len(df)) * 100).round(2)\n",
    "        column_min = df.min().round(2)\n",
    "        column_mean = df.mean().round(2)\n",
    "        column_max = df.max().round(2)\n",
    "        column_median = df.median().round(2)\n",
    "    \n",
    "    \n",
    "        # Create a Series for the names of the sim-vectors\n",
    "        file_name_series = pd.Series([csv_file], index=['file_name'])\n",
    "\n",
    "        # Create a Series for non_nan_columns_count\n",
    "        non_nan_columns_count_series = pd.Series([non_nan_columns_count], index=['non_nan_columns_count'])\n",
    "\n",
    "        # Concatenate statistics into a feature vector\n",
    "        feature_vector = pd.concat([file_name_series,non_nan_columns_count_series,nan_percentage,column_min, column_mean, column_max, column_median], axis=0)\n",
    "\n",
    "        # Replace NaN values in the feature vector with a specific numerical value\n",
    "        feature_vector.fillna(nan_replacement_value, inplace=True)\n",
    "\n",
    "        # Convert the feature vector to a list and append it to the feature vectors list\n",
    "        feature_vectors_list.append(feature_vector.tolist())\n",
    "    \n",
    "    # Convert the list of lists to a DataFrame\n",
    "    df = pd.DataFrame(feature_vectors_list, columns = ['compared_resources', 'Numer_of_columns_with_at_least_one_values', \n",
    "                                                       'MPN_Liste_perc_nan', 'EAN_Liste_perc_nan', 'Produktname_dic3_perc_nan',\n",
    "                                                       'MPN_Liste_min', 'EAN_Liste_min', 'Produktname_dic3_min', \n",
    "                                                       'MPN_Liste_mean', 'EAN_Liste_mean', 'Produktname_dic3_mean',\n",
    "                                                       'MPN_Liste_max', 'EAN_Liste_max','Produktname_dic3_max', \n",
    "                                                       'MPN_Liste_median', 'EAN_Liste_median', 'Produktname_dic3_median'])\n",
    "\n",
    "\n",
    "    # Order the columns in the dataframe according to the following order\n",
    "    columns_to_order = ['compared_resources', 'Numer_of_columns_with_at_least_one_values', \n",
    "                        'MPN_Liste_perc_nan', 'MPN_Liste_min', 'MPN_Liste_mean', 'MPN_Liste_max','MPN_Liste_median',\n",
    "                        'EAN_Liste_perc_nan', 'EAN_Liste_min', 'EAN_Liste_mean', 'EAN_Liste_max','EAN_Liste_median',\n",
    "                        'Produktname_dic3_perc_nan', 'Produktname_dic3_min', 'Produktname_dic3_mean', 'Produktname_dic3_max', 'Produktname_dic3_median'                                           \n",
    "                        ]\n",
    "\n",
    "    df = df[columns_to_order]\n",
    "    df.to_csv(os.path.join(path_to_feature_vectors_folder,'feature_vectors_init_features.csv'))\n",
    "\n",
    "    return df,df['compared_resources']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a4132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_representation_better_features():\n",
    "    \n",
    "    # Checks if the feature vector file already exists and if it exits , then remove it\n",
    "    delete_files_from_folder(path_to_feature_vectors_folder,'feature_vectors_better_features.csv')\n",
    "     \n",
    "    # Get a list of all CSV files in the folder \n",
    "    csv_files = [file for file in os.listdir(path_to_sim_vector_folder) if file.endswith('.csv')]\n",
    "\n",
    "    # Create an empty list to store feature vectors\n",
    "    feature_vectors_list = []\n",
    "\n",
    "    # Value to exclude from feature extraction\n",
    "    nan_replacement_value = 2 \n",
    "\n",
    "    # Columns to extracte from feature extraction \n",
    "    to_consider_columns = ['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "                           'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30', 'Breite_NumMaxProz30', \n",
    "                           'HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3']\n",
    "\n",
    "\n",
    "    # Iterate through each CSV file and generate statistics\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(path_to_sim_vector_folder, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df[to_consider_columns]\n",
    "        # convert \"/\" into Nan\n",
    "        df = df.apply(pd.to_numeric, errors= 'coerce')\n",
    "\n",
    "\n",
    "        # Calculate statistics\n",
    "        # The number of columns that have at least one non-Nan Value\n",
    "        #non_nan_columns_count = np.sum(df.notnull().any())\n",
    "        # The percentage of the Nan values in each column \n",
    "        nan_percentage = ((df.isnull().sum() / len(df)) * 100).round(2)\n",
    "        column_std_deviation = df.std().round(2)\n",
    "        column_median = df.median().round(2)\n",
    "        column_skewness = df.skew().round(2)\n",
    "        column_kurtosis = df.kurt().round(2)\n",
    "\n",
    "\n",
    "        # Create a Series for the names of the sim-vectors\n",
    "        file_name_series = pd.Series([csv_file], index=['file_name'])\n",
    "\n",
    "        # Create a Series for non_nan_columns_count\n",
    "        #non_nan_columns_count_series = pd.Series([non_nan_columns_count], index=['non_nan_columns_count'])\n",
    "\n",
    "        # Concatenate statistics into a feature vector\n",
    "        feature_vector = pd.concat([file_name_series,nan_percentage,column_std_deviation, column_median, column_skewness, column_kurtosis], axis=0)\n",
    "\n",
    "        # Replace NaN values in the feature vector with a specific numerical value\n",
    "        feature_vector.fillna(nan_replacement_value, inplace=True)\n",
    "\n",
    "        # Convert the feature vector to a list and append it to the feature vectors list\n",
    "        feature_vectors_list.append(feature_vector.tolist())\n",
    "\n",
    "    \n",
    "    \n",
    "    # Convert the list of lists to a DataFrame\n",
    "    df = pd.DataFrame(feature_vectors_list, columns = ['compared_resources', \n",
    "                                                       'MPN_Liste_perc_nan', 'EAN_Liste_perc_nan', 'Produktname_dic3_perc_nan',\n",
    "                                                       'Modell_Liste_perc_nan', 'Digital_zoom_perc_nan', 'optischer_zoom_perc_nan',\n",
    "                                                       'Breite_perc_nan', 'HÃ¶he_perc_nan', 'Gewicht_perc_nan','Sensortyp_perc_nan',\n",
    "                                                       'MPN_Liste_std_deviation', 'EAN_Liste_std_deviation', 'Produktname_dic3_std_deviation', 'Modell_Liste_std_deviation', \n",
    "                                                       'Digital_zoom_std_deviation', 'optischer_zoom_std_deviation', 'Breite_std_deviation', 'HÃ¶he_std_deviation',\n",
    "                                                       'Gewicht_std_deviation', 'Sensortyp_std_deviation', 'MPN_Liste_median', 'EAN_Liste_median', 'Produktname_dic3_median',\n",
    "                                                       'Modell_Liste_median', 'Digital_zoom_median','optischer_zoom_median', 'Breite_median',\n",
    "                                                       'HÃ¶he_median','Gewicht_median','Sensortyp_median', 'MPN_Liste_skewness', 'EAN_Liste_skewness',\n",
    "                                                       'Produktname_dic3_skewness', 'Modell_Liste_skewness', 'Digital_zoom_skewness', 'optischer_zoom_skewness',\n",
    "                                                       'Breite_skewness', 'HÃ¶he_skewness', 'Gewicht_skewness', 'Sensortyp_skewness',\n",
    "                                                       'MPN_Liste_kurtosis', 'EAN_Liste_kurtosis', 'Produktname_dic3_kurtosis', 'Modell_Liste_kurtosis',\n",
    "                                                       'Digital_zoom_kurtosis', 'optischer_zoom_kurtosis','Breite_kurtosis','HÃ¶he_kurtosis','Gewicht_kurtosis','Sensortyp_kurtosis'])\n",
    "\n",
    "\n",
    "    # Order the columns in the dataframe according to the following order\n",
    "    columns_to_order = ['compared_resources',\n",
    "                        'MPN_Liste_perc_nan', 'MPN_Liste_std_deviation', 'MPN_Liste_median', 'MPN_Liste_skewness','MPN_Liste_kurtosis',\n",
    "                        'EAN_Liste_perc_nan', 'EAN_Liste_std_deviation', 'EAN_Liste_median', 'EAN_Liste_skewness','EAN_Liste_kurtosis',\n",
    "                        'Produktname_dic3_perc_nan', 'Produktname_dic3_std_deviation', 'Produktname_dic3_median', 'Produktname_dic3_skewness', 'Produktname_dic3_kurtosis',\n",
    "                        'Modell_Liste_perc_nan', 'Modell_Liste_std_deviation', 'Modell_Liste_median', 'Modell_Liste_skewness', 'Modell_Liste_kurtosis',\n",
    "                        'Digital_zoom_perc_nan', 'Digital_zoom_std_deviation', 'Modell_Liste_median', 'Digital_zoom_skewness', 'Digital_zoom_kurtosis',\n",
    "                        'optischer_zoom_perc_nan', 'optischer_zoom_std_deviation', 'optischer_zoom_median', 'optischer_zoom_skewness','optischer_zoom_kurtosis',\n",
    "                        'Breite_perc_nan', 'Breite_std_deviation', 'Breite_median', 'Breite_skewness', 'Breite_kurtosis',\n",
    "                        'HÃ¶he_perc_nan', 'HÃ¶he_std_deviation', 'HÃ¶he_median', 'HÃ¶he_skewness', 'HÃ¶he_kurtosis',\n",
    "                        'Gewicht_perc_nan', 'Gewicht_std_deviation', 'Gewicht_median', 'Gewicht_skewness', 'Gewicht_kurtosis',\n",
    "                        'Sensortyp_perc_nan', 'Sensortyp_std_deviation', 'Sensortyp_median', 'Sensortyp_skewness', 'Sensortyp_kurtosis']       \n",
    "\n",
    "\n",
    "\n",
    "    df = df[columns_to_order]\n",
    "    df.to_csv(os.path.join(path_to_feature_vectors_folder, 'feature_vectors_better_features.csv'))\n",
    "\n",
    "    return df,df['compared_resources']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e433b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_representations_combined_features():\n",
    "    \n",
    "    # Checks if the feature vector file already exists and if it exits , then remove it\n",
    "    delete_files_from_folder(path_to_feature_vectors_folder,'feature_vectors_combined_features.csv')\n",
    "    \n",
    "    # Get a list of all CSV files in the folder \n",
    "    csv_files = [file for file in os.listdir(path_to_sim_vector_folder) if file.endswith('.csv')]\n",
    "    \n",
    "    # Create an empty list to store feature vectors\n",
    "    feature_vectors_list = []\n",
    "\n",
    "    # Value to exclude from feature extraction\n",
    "    nan_replacement_value = 99999 \n",
    "\n",
    "    # Columns to extracte from feature extraction \n",
    "    to_consider_columns = ['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "                           'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30', 'Breite_NumMaxProz30', \n",
    "                           'HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3']\n",
    "\n",
    "\n",
    "    # Iterate through each CSV file and generate statistics\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(path_to_sim_vector_folder, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df[to_consider_columns]\n",
    "        # convert \"/\" into Nan\n",
    "        df = df.apply(pd.to_numeric, errors= 'coerce')\n",
    "\n",
    "\n",
    "        # Calculate statistics\n",
    "        # The number of columns that have at least one non-Nan Value\n",
    "        non_nan_columns_count = np.sum(df.notnull().any())\n",
    "        # The percentage of the Nan values in each column \n",
    "        nan_percentage = ((df.isnull().sum() / len(df)) * 100).round(2)\n",
    "        column_min = df.min().round(2)\n",
    "        column_mean = df.mean().round(2)\n",
    "        column_max = df.max().round(2)\n",
    "        column_median = df.median().round(2)\n",
    "        \n",
    "        column_std_deviation = df.std().round(2)\n",
    "        column_skewness = df.skew().round(2)\n",
    "        column_kurtosis = df.kurt().round(2)\n",
    "\n",
    "        # Create a Series for the names of the sim-vectors\n",
    "        file_name_series = pd.Series([csv_file], index=['file_name'])\n",
    "\n",
    "        # Create a Series for non_nan_columns_count\n",
    "        non_nan_columns_count_series = pd.Series([non_nan_columns_count], index=['non_nan_columns_count'])\n",
    "\n",
    "        # Concatenate statistics into a feature vector\n",
    "        feature_vector = pd.concat([file_name_series,non_nan_columns_count_series,nan_percentage,column_min, column_mean, column_max, column_median], axis=0)\n",
    "\n",
    "        # Replace NaN values in the feature vector with a specific numerical value\n",
    "        feature_vector.fillna(nan_replacement_value, inplace=True)\n",
    "\n",
    "        # Convert the feature vector to a list and append it to the feature vectors list\n",
    "        feature_vectors_list.append(feature_vector.tolist())\n",
    "    \n",
    "    # Convert the list of lists to a DataFrame\n",
    "    df = pd.DataFrame(feature_vectors_list, columns = ['compared_resources', 'Numer_of_columns_with_at_least_one_values', \n",
    "                                                       'MPN_Liste_perc_nan', 'EAN_Liste_perc_nan', 'Produktname_dic3_perc_nan',\n",
    "                                                       'Modell_Liste_perc_nan', 'Digital_zoom_perc_nan', 'optischer_zoom_perc_nan',\n",
    "                                                       'Breite_perc_nan', 'HÃ¶he_perc_nan', 'Gewicht_perc_nan','Sensortyp_perc_nan',\n",
    "                                                       'MPN_Liste_min', 'EAN_Liste_min', 'Produktname_dic3_min', 'Modell_Liste_min', \n",
    "                                                       'Digital_zoom_min', 'optischer_zoom_min', 'Breite_min', 'HÃ¶he_min',\n",
    "                                                       'Gewicht_min', 'Sensortyp_min', 'MPN_Liste_mean', 'EAN_Liste_mean', 'Produktname_dic3_mean',\n",
    "                                                       'Modell_Liste_mean', 'Digital_zoom_mean','optischer_zoom_mean', 'Breite_mean',\n",
    "                                                       'HÃ¶he_mean','Gewicht_mean','Sensortyp_mean', 'MPN_Liste_max', 'EAN_Liste_max',\n",
    "                                                       'Produktname_dic3_max', 'Modell_Liste_max', 'Digital_zoom_max', 'optischer_zoom_max',\n",
    "                                                       'Breite_max', 'HÃ¶he_max', 'Gewicht_max', 'Sensortyp_max',\n",
    "                                                       'MPN_Liste_median', 'EAN_Liste_median', 'Produktname_dic3_median', 'Modell_Liste_median',\n",
    "                                                       'Digital_zoom_median', 'optischer_zoom_median','Breite_median','HÃ¶he_median','Gewicht_median','Sensortyp_median',\n",
    "                                                       'MPN_Liste_std_deviation', 'EAN_Liste_std_deviation', 'Produktname_dic3_std_deviation', \n",
    "                                                       'Modell_Liste_std_deviation', 'Digital_zoom_std_deviation', 'optischer_zoom_std_deviation',\n",
    "                                                       'Breite_std_deviation', 'HÃ¶he_std_deviation', 'Gewicht_std_deviation', \n",
    "                                                       'Sensortyp_std_devaition','MPN_Liste_skewness', 'EAN_Liste_skewness', \n",
    "                                                       'Produktname_dic3_skewness', 'Modell_Liste_skewness', 'Digital_zoom_skewness',\n",
    "                                                       'optischer_zoom_skewness', 'Breite_skewness', 'HÃ¶he_skewness',\n",
    "                                                       'Gewicht_skewness', 'Sensortyp_skewness','MPN_Liste_kurtosis',\n",
    "                                                       'EAN_Liste_kurtosis', 'Produktname_dic3_std_kurtosis', 'Modell_Liste_kurtosis',\n",
    "                                                       'Digital_zoom_kurtosis', 'optischer_zoom_kurtosis','Breite_kurtosis',\n",
    "                                                       'HÃ¶he_kurtosis', 'Gewicht_kurtosis', 'Sensortyp_kurtosis'])\n",
    "\n",
    "\n",
    "    # Order the columns in the dataframe according to the following order\n",
    "    columns_to_order = ['compared_resources', 'Numer_of_columns_with_at_least_one_values', \n",
    "                        'MPN_Liste_perc_nan', 'MPN_Liste_min', 'MPN_Liste_mean', 'MPN_Liste_max','MPN_Liste_median','MPN_Liste_std_deviation','MPN_Liste_skewness','MPN_Liste_kurtosis',\n",
    "                        'EAN_Liste_perc_nan', 'EAN_Liste_min', 'EAN_Liste_mean', 'EAN_Liste_max','EAN_Liste_median','EAN_Liste_std_deviation','EAN_Liste_skewness','EAN_Liste_kurtosis',\n",
    "                        'Produktname_dic3_perc_nan', 'Produktname_dic3_min', 'Produktname_dic3_mean', 'Produktname_dic3_max', 'Produktname_dic3_median','Produktname_dic3_std_deviation','Produktname_dic3_skewness','Produktname_dic3_std_kurtosis',\n",
    "                        'Modell_Liste_perc_nan', 'Modell_Liste_min', 'Modell_Liste_mean', 'Modell_Liste_max', 'Modell_Liste_median','Modell_Liste_std_deviation','Modell_Liste_skewness','Modell_Liste_kurtosis',\n",
    "                        'Digital_zoom_perc_nan', 'Digital_zoom_min', 'Digital_zoom_mean', 'Digital_zoom_max', 'Digital_zoom_median','Digital_zoom_std_deviation','Digital_zoom_skewness','Digital_zoom_kurtosis',\n",
    "                        'optischer_zoom_perc_nan', 'optischer_zoom_min', 'optischer_zoom_mean', 'optischer_zoom_max','optischer_zoom_median','optischer_zoom_std_deviation','optischer_zoom_skewness','optischer_zoom_kurtosis',\n",
    "                        'Breite_perc_nan', 'Breite_min', 'Breite_mean', 'Breite_max', 'Breite_median','Breite_std_deviation','Breite_skewness','Breite_kurtosis',\n",
    "                        'HÃ¶he_perc_nan', 'HÃ¶he_min', 'HÃ¶he_mean', 'HÃ¶he_max', 'HÃ¶he_median', 'HÃ¶he_std_deviation', 'HÃ¶he_skewness', 'HÃ¶he_kurtosis'\n",
    "                        'Gewicht_perc_nan', 'Gewicht_min', 'Gewicht_mean', 'Gewicht_max', 'Gewicht_median','Gewicht_std_deviation', 'Gewicht_skewness','Gewicht_kurtosis'\n",
    "                        'Sensortyp_perc_nan', 'Sensortyp_min', 'Sensortyp_mean', 'Sensortyp_max', 'Sensortyp_median', 'Sensortyp_std_devaition','Sensortyp_skewness','Sensortyp_kurtosis'                                            \n",
    "                        ]\n",
    "\n",
    "    df = df[columns_to_order]\n",
    "    df.to_csv(os.path.join(path_to_feature_vectors_folder,'feature_vectors_combined_features.csv'))\n",
    "\n",
    "    return df,df['compared_resources']\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bdf321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all CSV files in the folder \n",
    "csv_files = [file for file in os.listdir(path_to_sim_vector_folder) if file.endswith('.csv')]\n",
    "    \n",
    "# Create an empty list to store feature vectors\n",
    "feature_vectors_list = []\n",
    "\n",
    "# Value to exclude from feature extraction\n",
    "nan_replacement_value = 2 \n",
    "\n",
    "# Columns to extracte from feature extraction \n",
    "to_consider_columns = ['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "                       'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30', 'Breite_NumMaxProz30', \n",
    "                       'HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3']\n",
    "\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(path_to_sim_vector_folder, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[to_consider_columns]\n",
    "    # convert \"/\" into Nan\n",
    "    df = df.apply(pd.to_numeric, errors= 'coerce')\n",
    "    # Replace NaN values in the feature vector with a specific numerical value\n",
    "    df.fillna(nan_replacement_value, inplace=True)\n",
    "    \n",
    "    # Standardize the data (important for PCA)\n",
    "    scaler = StandardScaler()\n",
    "    df_standardized = scaler.fit_transform(df)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA()\n",
    "    df_pca = pca.fit_transform(df_standardized)\n",
    "\n",
    "    # Create a DataFrame with principal components\n",
    "    df_pca_result = pd.DataFrame(data=df_pca, columns=['PC1', 'PC2', 'PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10'])\n",
    "\n",
    "    # Display the DataFrame with principal components\n",
    "    print(df_pca_result)\n",
    "\n",
    "    # Explained variance ratio\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "    \n",
    "    \n",
    "    # Plot explained variance\n",
    "    plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7709bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'Feature1': [1, 2, 3, 4, 5],\n",
    "        'Feature2': [5, 4, 3, 2, 1],\n",
    "        'Feature3': [2, 3, 1, 4, 5]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standardize the data (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "df_standardized = scaler.fit_transform(df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "df_pca = pca.fit_transform(df_standardized)\n",
    "\n",
    "# Create a DataFrame with principal components\n",
    "df_pca_result = pd.DataFrame(data=df_pca, columns=['PC1', 'PC2', 'PC3'])\n",
    "\n",
    "# Display the DataFrame with principal components\n",
    "print(df_pca_result)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "\n",
    "# Plot explained variance\n",
    "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49bc847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Generate two synthetic datasets\n",
    "np.random.seed(42)\n",
    "data_x = np.random.randn(100, 2)  # Samples from the first distribution\n",
    "data_y = np.random.randn(100, 2) + 2  # Samples from the second distribution (shifted)\n",
    "\n",
    "# Perform MMD test\n",
    "mmd_test = MMD_2_Sample_Test(data_x, data_y)\n",
    "result = mmd_test.perform_test()\n",
    "\n",
    "# Display the test result\n",
    "print(\"MMD Statistic:\", result['testStat'])\n",
    "print(\"P-value:\", result['pValue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7d12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_representations_mean_embeddings():\n",
    "    \n",
    "    # Checks if the feature vector file already exists and if it exits , then remove it\n",
    "    delete_files_from_folder(path_to_feature_vectors_folder,'feature_vectors_mean_embeddings_features.csv')\n",
    "    \n",
    "    # Get a list of all CSV files in the folder \n",
    "    csv_files = [file for file in os.listdir(path_to_sim_vector_folder) if file.endswith('.csv')]\n",
    "    \n",
    "    # Create an empty list to store feature vectors\n",
    "    feature_vectors_list = []\n",
    "\n",
    "    # Value to exclude from feature extraction\n",
    "    nan_replacement_value = 2 \n",
    "\n",
    "    # Columns to extracte from feature extraction \n",
    "    to_consider_columns = ['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "                           'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30', 'Breite_NumMaxProz30', \n",
    "                           'HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3']\n",
    "\n",
    "\n",
    "    # Iterate through each CSV file and generate statistics\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(path_to_sim_vector_folder, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df[to_consider_columns]\n",
    "        # convert \"/\" into Nan\n",
    "        df = df.apply(pd.to_numeric, errors= 'coerce')\n",
    "        # Replace NaN values in the feature vector with a specific numerical value\n",
    "        df.fillna(nan_replacement_value, inplace=True)\n",
    "        df = df.values\n",
    "        rbf_feature = RBFSampler(gamma=1, random_state=1)\n",
    "        df_rbf = rbf_feature.fit_transform(df)\n",
    "        # Calculate MMD\n",
    "        mmd = df_rbf.mean(axis=0)      \n",
    "        # Convert the mmd np-array into a list\n",
    "        mmd = mmd.tolist()\n",
    "        # Insert the compared_resource name at the first index of the list \n",
    "        mmd.insert(0, csv_file)\n",
    "        # Convert the feature vector to a list and append it to the feature vectors list\n",
    "        feature_vectors_list.append(mmd)\n",
    "        \n",
    "        \n",
    "    # Convert the list of lists to a DataFrame\n",
    "    df = pd.DataFrame(feature_vectors_list)\n",
    "    # Assign names to columns\n",
    "    column_names = [f\"feature_{i+1}\" for i in range(101)]\n",
    "    df.columns = column_names\n",
    "    df.to_csv(os.path.join(path_to_feature_vectors_folder,'feature_vectors_mean_embeddings_features.csv'))\n",
    "    print(df.head())\n",
    "    return df,df['feature_1']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bfbfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # specify the path of the data folder \n",
    "    path_to_data_folder = '/Users/abdulnaser/Desktop/Masterarbeit/metadatatransferlearning-main/meta_tl/data/'\n",
    "\n",
    "    # Specify the folder path containing the csv files \n",
    "    path_to_sim_vector_folder =  path_to_data_folder + 'sim_dataframes/'\n",
    "\n",
    "    # Get a list of all CSV files in the folder \n",
    "    csv_files = [file for file in os.listdir(path_to_sim_vector_folder) if file.endswith('.csv')]\n",
    "\n",
    "\n",
    "    # Create an empty list to store feature vectors\n",
    "    feature_vectors_list = []\n",
    "\n",
    "    # Value to exclude from feature extraction\n",
    "    nan_replacement_value = 10 \n",
    "\n",
    "    # Columns to extracte from feature extraction \n",
    "    to_consider_columns = ['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "                           'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30', 'Breite_NumMaxProz30', \n",
    "                           'HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3']\n",
    "\n",
    "\n",
    "    # Iterate through each CSV file and generate statistics\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(path_to_sim_vector_folder, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df[to_consider_columns]\n",
    "        # convert \"/\" into Nan\n",
    "        df = df.apply(pd.to_numeric, errors= 'coerce')\n",
    "\n",
    "\n",
    "        # Calculate statistics\n",
    "        # The number of columns that have at least one non-Nan Value\n",
    "        non_nan_columns_count = np.sum(df.notnull().any())\n",
    "        # The percentage of the Nan values in each column \n",
    "        nan_percentage = ((df.isnull().sum() / len(df)) * 100).round(2)\n",
    "        column_min = df.min().round(2)\n",
    "        column_mean = df.mean().round(2)\n",
    "        column_max = df.max().round(2)\n",
    "        column_median = df.median().round(2)\n",
    "\n",
    "\n",
    "        # Create a Series for the names of the sim-vectors\n",
    "        file_name_series = pd.Series([csv_file], index=['file_name'])\n",
    "\n",
    "        # Create a Series for non_nan_columns_count\n",
    "        non_nan_columns_count_series = pd.Series([non_nan_columns_count], index=['non_nan_columns_count'])\n",
    "\n",
    "        # Concatenate statistics into a feature vector\n",
    "        feature_vector = pd.concat([file_name_series,non_nan_columns_count_series,nan_percentage,column_min, column_mean, column_max, column_median], axis=0)\n",
    "\n",
    "        # Replace NaN values in the feature vector with a specific numerical value\n",
    "        feature_vector.fillna(nan_replacement_value, inplace=True)\n",
    "\n",
    "        # Convert the feature vector to a list and append it to the feature vectors list\n",
    "        feature_vectors_list.append(feature_vector.tolist())\n",
    "    \n",
    "    # Convert the list of lists to a DataFrame\n",
    "    df = pd.DataFrame(feature_vectors_list, columns = ['compared_resources', 'Numer_of_columns_with_at_least_one_values', \n",
    "                                                      'MPN_Liste_perc_nan', 'EAN_Liste_perc_nan', 'Produktname_dic3_perc_nan',\n",
    "                                                      'Modell_Liste_perc_nan', 'Digital_zoom_perc_nan', 'optischer_zoom_perc_nan',\n",
    "                                                      'Breite_perc_nan', 'HÃ¶he_perc_nan', 'Gewicht_perc_nan','Sensortyp_perc_nan',\n",
    "                                                      'MPN_Liste_min', 'EAN_Liste_min', 'Produktname_dic3_min', 'Modell_Liste_min', \n",
    "                                                      'Digital_zoom_min', 'optischer_zoom_min', 'Breite_min', 'HÃ¶he_min',\n",
    "                                                      'Gewicht_min', 'Sensortyp_min', 'MPN_Liste_mean', 'EAN_Liste_mean', 'Produktname_dic3_mean',\n",
    "                                                      'Modell_Liste_mean', 'Digital_zoom_mean','optischer_zoom_mean', 'Breite_mean',\n",
    "                                                      'HÃ¶he_mean','Gewicht_mean','Sensortyp_mean', 'MPN_Liste_max', 'EAN_Liste_max',\n",
    "                                                      'Produktname_dic3_max', 'Modell_Liste_max', 'Digital_zoom_max', 'optischer_zoom_max',\n",
    "                                                      'Breite_max', 'HÃ¶he_max', 'Gewicht_max', 'Sensortyp_max',\n",
    "                                                      'MPN_Liste_median', 'EAN_Liste_median', 'Produktname_dic3_median', 'Modell_Liste_median',\n",
    "                                                      'Digital_zoom_median', 'optischer_zoom_median','Breite_median','HÃ¶he_median','Gewicht_median','Sensortyp_median'])\n",
    "\n",
    "\n",
    "    # Order the columns in the dataframe according to the following order\n",
    "    columns_to_order = ['compared_resources', 'Numer_of_columns_with_at_least_one_values', \n",
    "                        'MPN_Liste_perc_nan', 'MPN_Liste_min', 'MPN_Liste_mean', 'MPN_Liste_max','MPN_Liste_median',\n",
    "                        'EAN_Liste_perc_nan', 'EAN_Liste_min', 'EAN_Liste_mean', 'EAN_Liste_max','EAN_Liste_median',\n",
    "                        'Produktname_dic3_perc_nan', 'Produktname_dic3_min', 'Produktname_dic3_mean', 'Produktname_dic3_max', 'Produktname_dic3_median',\n",
    "                        'Modell_Liste_perc_nan', 'Modell_Liste_min', 'Modell_Liste_mean', 'Modell_Liste_max', 'Modell_Liste_median',\n",
    "                        'Digital_zoom_perc_nan', 'Digital_zoom_min', 'Digital_zoom_mean', 'Digital_zoom_max', 'Digital_zoom_median',\n",
    "                        'optischer_zoom_perc_nan', 'optischer_zoom_min', 'optischer_zoom_mean', 'optischer_zoom_max','optischer_zoom_median',\n",
    "                        'Breite_perc_nan', 'Breite_min', 'Breite_mean', 'Breite_max', 'Breite_median',\n",
    "                        'HÃ¶he_perc_nan', 'HÃ¶he_min', 'HÃ¶he_mean', 'HÃ¶he_max', 'HÃ¶he_median',\n",
    "                        'Gewicht_perc_nan', 'Gewicht_min', 'Gewicht_mean', 'Gewicht_max', 'Gewicht_median',\n",
    "                        'Sensortyp_perc_nan', 'Sensortyp_min', 'Sensortyp_mean', 'Sensortyp_max', 'Sensortyp_median'                                             \n",
    "                       ]\n",
    "\n",
    "    df = df[columns_to_order]\n",
    "\n",
    "\n",
    "    # Multiple each Feature by its Weight\n",
    "    weights = [0,4,2,2,2,2,2,2,2,2,2,2,9,9,9,9,9,7,7,7,7,7,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3]\n",
    "\n",
    "    # Multiply each feature by its weight\n",
    "    df = df * weights\n",
    "\n",
    "    df.to_csv(path_to_data_folder + 'feature_vectors/' + 'feature_vectors.csv')\n",
    "\n",
    "    return df,df['compared_resources']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40946a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_meta_data_representations():\n",
    "    if initial_config['meta_data_representation'] == 1:\n",
    "         return generate_data_representation_initial_features()\n",
    "    elif initial_config['meta_data_representation'] == 2:\n",
    "         return generate_data_representation_better_features()\n",
    "    elif initial_config['meta_data_representation'] == 3:\n",
    "         return generate_data_representations_combined_features()\n",
    "    elif initial_config['meta_data_representation'] == 4:\n",
    "         return generate_data_representations_mean_embeddings()        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae331f",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a35bf4",
   "metadata": {},
   "source": [
    "# Selecting the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0f8ed",
   "metadata": {},
   "source": [
    "### 1) Silhouette Score \n",
    "the silhouette score is a metric used to calculate the goodness of a clusteringt algorithum. \n",
    "it measures how similair an object to its own cluster(cohesion) compared to other clusters(separation). The silhouette score ranges from -1 to 1, where a high value indicates that the object is well-matched to its own cluster and poorly matches to neighboring clusters. if most objects have a high vlaue , then the clustering configuration is approprite . if many points have a low or negatove value, then the clustering configuration may have too many or too few clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc72de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data , sources = generate_data_representations_mean_embeddings()\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_number_of_clusters_using_silhouette_score(data):\n",
    "    silh_scores = []\n",
    "    data_array = data\n",
    "\n",
    "    list_k = list(range(2,40))\n",
    "\n",
    "    for k in list_k:\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        clusters = kmeans.fit_predict(data_array)\n",
    "        silhouette_avg = silhouette_score(data_array, clusters)\n",
    "        silh_scores.append(silhouette_avg)\n",
    "\n",
    "    # Get the index of the highest number\n",
    "    index_of_highest_number = silh_scores.index(max(silh_scores)) + 1\n",
    "    highest_number = max(silh_scores)\n",
    "    print(\"The best number of clusters is {} with an average silhouette score of {}\".format(index_of_highest_number,highest_number))\n",
    "    \n",
    "    # Plot silh_scores against k \n",
    "    plt.figure(figsize=(6,10))\n",
    "    plt.plot(list_k, silh_scores, '-o')\n",
    "    plt.xlabel(r'Number of clusters *k*')\n",
    "    plt.ylabel('Avg Silhouette Scores')\n",
    "    \n",
    "    return index_of_highest_number, highest_number\n",
    "\n",
    "\n",
    "data , sources = generate_data_representation_very_new_test()\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_standardized = scaler.fit_transform(data.iloc[:,1:])\n",
    " \n",
    "get_best_number_of_clusters_using_silhouette_score(X_standardized)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data , sources = generate_data_representation_very_new_test()\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_standardized = scaler.fit_transform(data.iloc[:,1:])\n",
    "X_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45f3c0",
   "metadata": {},
   "source": [
    "### 2) WCSS stand for Within-CLuster Sum of sqaures\n",
    "It is a metric used to evaluate the performacne of a clustering algorithum , such as k-means. \n",
    "In the context of k-menas clustering, WCSS represnets the sum of squared distances between each \n",
    "data point in a cluster and the centroid of that cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e320f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_number_of_clusters_using_wcss_score(data):\n",
    "    wcss = []\n",
    "    data_array = data.iloc[:, 1:].values\n",
    "    for k in range(1, 20):  # You can adjust the range of k as needed\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(data_array)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    \n",
    "    # Get the index of the highest number\n",
    "    index_of_highest_number = wcss.index(min(wcss)) + 1\n",
    "    highest_number = min(wcss)\n",
    "    print(\"The best number of clusters is {} with an wcss score of {}\".format(index_of_highest_number,highest_number))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, 20), wcss, marker='o', linestyle='--')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.show()\n",
    "    \n",
    "    return index_of_highest_number,highest_number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cbd9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data , sources = generate_data_representation_very_new_test()\n",
    "# Fit and transform the data\n",
    "X_standardized = scaler.fit_transform(data.iloc[:,1:])\n",
    "get_best_number_of_clusters_using_wcss_score(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c44158",
   "metadata": {},
   "source": [
    "# Predict the clusters for each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(df,num_of_clusters,feature_vectors_list):\n",
    "    # Number of clusters\n",
    "    n_clusters = num_of_clusters\n",
    "\n",
    "    # Run KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(df.iloc[:, 1:].values)\n",
    "\n",
    "\n",
    "    # Organize strings based on their cluster assignments\n",
    "    cluster_strings = {}\n",
    "    print(kmeans.labels_)\n",
    "    for i, cluster in enumerate(kmeans.labels_):\n",
    "        string = feature_vectors_list[i]\n",
    "        if cluster not in cluster_strings:\n",
    "            cluster_strings[cluster] = []\n",
    "        cluster_strings[cluster].append(string)\n",
    "\n",
    "\n",
    "    \n",
    "    # Print cluster number and strings belonging to each other\n",
    "    for cluster, strings in cluster_strings.items():\n",
    "        print(f\"Cluster Number: {cluster}\")\n",
    "        print(f\"Elements: {', '.join(strings)}\")\n",
    "        print()\n",
    "        \n",
    "    clusters = kmeans.labels_\n",
    "\n",
    "    # Add the cluster assignments to the DataFrame\n",
    "    df['Cluster'] = clusters\n",
    "\n",
    "    # Count the occurrences of each cluster \n",
    "    cluster_counts = np.bincount(clusters)\n",
    "\n",
    "    # Print the cluster counts \n",
    "    print(\"\\nNumber of elements in each cluster:\")\n",
    "    for cluster_num, count in enumerate(cluster_counts):\n",
    "        print(f\"Cluster {cluster_num}: {count} elements\")\n",
    "        \n",
    "    return cluster_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec1bed",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b38c9",
   "metadata": {},
   "source": [
    "# 1. Create XGBoost models trained on the largest data source in each cluster and save them in the models folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_models(cluster_strings):\n",
    "    # Remvoe already created models\n",
    "    delete_files_from_folder(path_to_models_folders)\n",
    "    cluster_max_row_file = {}\n",
    "    # 1. Extract the file with the most number of rows \n",
    "    for cluster,files in cluster_strings.items():\n",
    "        # Initialize variable to keep track of the file with the most rows \n",
    "        max_rows = 0\n",
    "        max_rows_file = \"\"\n",
    "\n",
    "        # Iterate through the CSV files and find the one with the most rows\n",
    "        for file in files:\n",
    "            try:\n",
    "                # Read the CSV_file\n",
    "                df = pd.read_csv(path_to_sim_vector_folder + file)\n",
    "\n",
    "                # Get the number of rows in the dataframe\n",
    "                num_rows = len(df)\n",
    "\n",
    "                # compare the number of rows with the current maximum\n",
    "                if num_rows > max_rows:\n",
    "                    max_rows = num_rows\n",
    "                    max_rows_file = file\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "        print(\"cluster\")\n",
    "        print(cluster)\n",
    "        print(\"max_file\")\n",
    "        print(max_rows_file)\n",
    "    \n",
    "        # 2. Build an XGBoost model and train it on the data of the dataframe with the most number of rows \n",
    "        sim_vector_file_with_highest_number = pd.read_csv(path_to_sim_vector_folder + max_rows_file)\n",
    "\n",
    "        print(\"shape\")\n",
    "        print(sim_vector_file_with_highest_number.shape)\n",
    "        print(\"count values is:\")\n",
    "        print(sim_vector_file_with_highest_number['is_match'].value_counts())\n",
    "        # Drop the specified columns using the drop() function\n",
    "        sim_vector_file_with_highest_number.drop(columns=['record_compared_1','record_compared_2','Modell_no_Liste_TruncateBegin20','Unnamed: 0','recId','recId.1'], axis=1, inplace=True)\n",
    "\n",
    "        # Replace \"/\" with 9999 in 'is_match' column\n",
    "        sim_vector_file_with_highest_number.replace('/', 9999, inplace=True)\n",
    "\n",
    "        # Convert all columns to numerical data types \n",
    "        sim_vector_file_with_highest_number = sim_vector_file_with_highest_number.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        if initial_config['training_instance_based'] == True:  \n",
    "            sim_vector_file_with_highest_number['Produktname_dic3'] = sim_vector_file_with_highest_number['Produktname_dic3'].round(2)\n",
    "            sim_vector_file_with_highest_number['Modell_Liste_3g'] = sim_vector_file_with_highest_number['Modell_Liste_3g'].round(2)\n",
    "            columns_to_select = [col for col in sim_vector_file_with_highest_number.columns if col != 'is_match'] \n",
    "            sim_vector_file_with_highest_number['closest_match_ratio'] = sim_vector_file_with_highest_number.apply(lambda row: calcualte_ratio(row,sim_vector_file_with_highest_number,columns_to_select), axis=1)\n",
    "            sim_vector_file_with_highest_number = sim_vector_file_with_highest_number.loc[sim_vector_file_with_highest_number['closest_match_ratio'] > 0.5]\n",
    "            sim_vector_file_with_highest_number.drop(columns=['closest_match_ratio'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        # Assuming the last column is the target variable and the rest are features\n",
    "        X = sim_vector_file_with_highest_number.iloc[:, :-1] # Features (all columns except the last one)\n",
    "        y = sim_vector_file_with_highest_number.iloc[: , -1] # Taregt variable (is_match)\n",
    "\n",
    "        # Create an XGBoost classifier \n",
    "        model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "        # Train the model in the taining data\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Specify the full path for saving the model \n",
    "        #model_path = path_to_modles_folder + 'Cluster_' + str(cluster) + '.model'\n",
    "\n",
    "        # Save the XGBoost model to the specified path\n",
    "        #model.save_model(model_path)\n",
    "        file_name = path_to_models_folders + \"Cluster_\" +str(cluster) + '.pkl'\n",
    "\n",
    "        # save\n",
    "        pickle.dump(model, open(file_name, \"wb\"))\n",
    "\n",
    "        cluster_max_row_file['Cluster_'+str(cluster)+'.pkl'] = max_rows_file\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922726d",
   "metadata": {},
   "source": [
    "# Train each sim_vec_file by the clusters model and evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_training(cluster_strings):\n",
    "    # List all files in the directory\n",
    "    model_files = [f for f in os.listdir(path_to_models_folders) if f.endswith('.pkl')]\n",
    "\n",
    "    # Load each XGBoost model \n",
    "    loaded_models = {}\n",
    "    sim_vec_file_to_process = []\n",
    "    trained_models = []\n",
    "    performance = [] \n",
    "\n",
    "    for model_file in model_files:\n",
    "        # Construct the full path for the model\n",
    "        model_path = os.path.join(path_to_models_folders, model_file)\n",
    "        # Load the XGBoost model\n",
    "        xgb_model_loaded = pickle.load(open(model_path,\"rb\"))\n",
    "\n",
    "        # Append the loaded model to the list associated with the model name\n",
    "        if model_file not in loaded_models:\n",
    "            loaded_models[model_file] = []\n",
    "        loaded_models[model_file].append(xgb_model_loaded)\n",
    "    \n",
    "    \n",
    "    # Get a list of all CSV files in the folder \n",
    "    csv_files = [file for file in os.listdir(path_to_sim_vector_folder) if file.endswith('.csv')]\n",
    "\n",
    "\n",
    "    for file_name in csv_files:\n",
    "       print(\"The file to process is {}\".format(file_name))\n",
    "       # find the first key associated with the specified value \n",
    "       first_matching_key = [key for key,value in cluster_strings.items() if file_name in value]\n",
    "       sim_cluster_name = ('Cluster_'+str(first_matching_key)+'.pkl').replace('[','').replace(']','')\n",
    "       print(\"The cluster name is {}\".format(sim_cluster_name))\n",
    "       full_path = os.path.join(path_to_sim_vector_folder, file_name)\n",
    "       sim_vec_file = pd.read_csv(full_path)\n",
    "       sim_vec_file.drop(columns=['record_compared_1','record_compared_2','Modell_no_Liste_TruncateBegin20','Unnamed: 0','recId','recId.1'], axis=1, inplace=True)\n",
    "       # Replace \"/\" with 9999 in 'is_match' column\n",
    "       sim_vec_file.replace('/', 9999, inplace=True)\n",
    "       # Convert all columns to numerical data types\n",
    "       sim_vec_file = sim_vec_file.apply(pd.to_numeric, errors='coerce')\n",
    "       X_test = sim_vec_file.iloc[:, :-1] # Features (all columns except the last one)\n",
    "       y_test = sim_vec_file.iloc[: , -1] # Taregt variable (is_match)\n",
    "\n",
    "\n",
    "       # Print cluster number and strings belonging to each other\n",
    "       for model_name, model in loaded_models.items():\n",
    "           if model_name == sim_cluster_name:\n",
    "              sim_vector_file_model = pd.read_csv(path_to_sim_vector_folder + cluster_max_row_file.get(model_name))\n",
    "              sim_vec_file_to_process.append(file_name)\n",
    "              trained_models.append(\"Original cluster model \" + cluster_max_row_file.get(model_name)) \n",
    "              predictions = model[0].predict(X_test)\n",
    "              class_probs = model[0].predict_proba(X_test)\n",
    "              sim_vec_file['pred'] = predictions\n",
    "              sim_vec_file[['probabilties_0', 'probabilties_1']] = class_probs\n",
    "              accuracy = accuracy_score(y_test, predictions)\n",
    "              performance.append(accuracy)\n",
    "              sim_vec_file.to_csv(active_learning_path + file_name + model_name + \".csv\") \n",
    "\n",
    "\n",
    "           else:\n",
    "              predictions = model[0].predict(X_test)\n",
    "              class_probs = model[0].predict_proba(X_test)\n",
    "              sim_vec_file['pred'] = predictions\n",
    "              sim_vec_file[['probabilties_0', 'probabilties_1']] = class_probs\n",
    "              accuracy = accuracy_score(y_test, predictions)\n",
    "              sim_vector_file_model = pd.read_csv(path_to_sim_vector_folder + cluster_max_row_file.get(model_name))\n",
    "              sim_vec_file_to_process.append(file_name)\n",
    "              trained_models.append(cluster_max_row_file.get(model_name))\n",
    "              performance.append(accuracy)\n",
    "              #sim_vec_file.to_csv(active_learning_path + file_name + model_name + \".csv\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = {'sim_vec_file':sim_vec_file_to_process, 'model':trained_models, 'performance': performance}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.head())\n",
    "    \n",
    "    file_name = \"\"\n",
    "    \n",
    "    if initial_config['data_duplicated']:\n",
    "        file_name = \"with_duplicated_data_\"\n",
    "    else: \n",
    "        file_name = \"without_duplicated_data_\"\n",
    "        \n",
    "    file_name = file_name + meta_data_rep_mapping[initial_config['meta_data_representation']] + \"_\"\n",
    "     \n",
    "    if initial_config['training_instance_based']:\n",
    "        file_name = file_name + 'instance_based.csv'\n",
    "    else:\n",
    "        file_name = file_name + 'not_instance_based.csv'\n",
    "    \n",
    "    df.to_csv(os.path.join(path_to_results_folder + file_name))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9bdabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_evaluation(): \n",
    "    # Get a list of all CSV files in the folder \n",
    "    results_csv_files = [file for file in os.listdir(path_to_results_folder) if file.endswith('.csv')]\n",
    "    for result_csv_file in results_csv_files:\n",
    "        result_file_path = os.path.join(path_to_results_folder, result_csv_file)\n",
    "        result_df = pd.read_csv(result_file_path)\n",
    "        print(\"The configuration is: \")\n",
    "        print(result_csv_file)\n",
    "        print(\"The mean performance is: \")\n",
    "        print(result_df['performance'].mean())\n",
    "\n",
    "#performance_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba69f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_rep,compared_resources = generate_data_representation_very_new()\n",
    "\n",
    "# specify the path of the data folder \n",
    "path_to_data_folder = '/Users/abdulnaser/Desktop/Masterarbeit/metadatatransferlearning-main/meta_tl/data/'\n",
    "\n",
    "# Specify the folder path containing the csv files \n",
    "path_to_sim_vector_folder =  path_to_data_folder + 'sim_dataframes/'\n",
    "\n",
    "# Get a list of all CSV files in the folder \n",
    "csv_files = [file for file in os.listdir(path_to_sim_vector_folder) if file.endswith('.csv')]\n",
    "\n",
    "# Columns to extracte from feature extraction \n",
    "to_consider_columns = ['Produktname_dic3','Modell_Liste_3g']\n",
    "\n",
    "\n",
    "first_file = []\n",
    "second_file = []\n",
    "similiarity_value = []\n",
    "\n",
    "for index_1 , row_1 in meta_data_rep.iterrows():\n",
    "    print(f\"First dataset is {csv_file_1} \")\n",
    "    file_path_1 = os.path.join(path_to_sim_vector_folder, row_1['compared_resources'])\n",
    "    file_df_1 = pd.read_csv(file_path_1)\n",
    "    file_df_1 = file_df_1[to_consider_columns]\n",
    "    file_df_1 = file_df_1.apply(pd.to_numeric, errors = 'coerce')\n",
    "    file_df_1.fillna(2,inplace=True)\n",
    "    \n",
    "    for index_2 , row_2 in meta_data_rep.iterrows():\n",
    "        file_path_2 = os.path.join(path_to_sim_vector_folder, row_2['compared_resources'])\n",
    "        if file_path_1 != file_path_2:\n",
    "            print(f\"second dataset is {csv_file_2}\")\n",
    "            first_file.append(row_1['compared_resources'])\n",
    "            second_file.append(row_2['compared_resources'])\n",
    "            file_df_2 = pd.read_csv(file_path_2)\n",
    "            file_df_2 = file_df_2[to_consider_columns]\n",
    "            file_df_2 = file_df_2.apply(pd.to_numeric, errors = 'coerce')\n",
    "            file_df_2.fillna(2,inplace=True)\n",
    "            similiarity_value.append(get_similairty_value(row_1.iloc[1:].tolist(),row_2.iloc[1:].tolist()))\n",
    "            \n",
    "            \n",
    "            \n",
    "df = pd.DataFrame({'first_file': first_file, 'second_file': second_file,'similiar':similiarity_value})\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "#get_similairty_value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['similiar']>0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfcb05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxmax_values = df.groupby(['first_file'])['similiar'].idxmax()\n",
    "result_updated = df.loc[idxmax_values]\n",
    "result_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b562b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = result_updated.groupby(['second_file']).count().reset_index()\n",
    "n.sort_values(by='similiar',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_updated.loc[(result_updated['second_file']=='www.buzzillions.com_cammarkt.com.csv')]['first_file'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_updated.loc[(result_updated['second_file']=='www.garricks.com.au_cammarkt.com.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22560734",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_updated.loc[(result_updated['second_file']=='www.gosale.com_www.eglobalcentral.co.uk.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['cammarkt.com_www.pcconnection.com.csv',\n",
    "           'www.wexphotographic.com_www.pcconnection.com.csv',\n",
    "           'www.eglobalcentral.co.uk_www.pcconnection.com.csv',\n",
    "           'www.eglobalcentral.co.uk_cammarkt.com.csv',\n",
    "           'www.wexphotographic.com_www.eglobalcentral.co.uk.csv',\n",
    "           'buy.net_www.wexphotographic.com.csv']\n",
    "\n",
    "\n",
    "# specify the path of the data folder \n",
    "path_to_data_folder = '/Users/abdulnaser/Desktop/Masterarbeit/metadatatransferlearning-main/meta_tl/data/'\n",
    "\n",
    "# Specify the folder path containing the csv files \n",
    "path_to_sim_vector_folder =  path_to_data_folder + 'sim_dataframes/'\n",
    "\n",
    "# Get a list of all CSV files in the folder \n",
    "csv_files = [file for file in os.listdir(path_to_sim_vector_folder) if file.endswith('.csv')]\n",
    "\n",
    "# Columns to extracte from feature extraction \n",
    "to_consider_columns = ['Produktname_dic3','Modell_Liste_3g']\n",
    "\n",
    "\n",
    "first_file = []\n",
    "second_file = []\n",
    "ks_statistc_produkt_name = []\n",
    "ks_statistic_model_list = []\n",
    "similiar = []\n",
    "\n",
    "for csv_file_1 in sources:\n",
    "    print(f\"First dataset is {csv_file_1} \")\n",
    "    file_path_1 = os.path.join(path_to_sim_vector_folder, csv_file_1)\n",
    "    file_df_1 = pd.read_csv(file_path_1)\n",
    "    file_df_1 = file_df_1[to_consider_columns]\n",
    "    file_df_1 = file_df_1.apply(pd.to_numeric, errors = 'coerce')\n",
    "    #file_df_1.fillna(2,inplace=True)\n",
    "    \n",
    "    for csv_file_2 in sources:\n",
    "        file_path_2 = os.path.join(path_to_sim_vector_folder, csv_file_2)\n",
    "        if file_path_1 != file_path_2:\n",
    "            print(f\"second dataset is {csv_file_2}\")\n",
    "            first_file.append(csv_file_1)\n",
    "            second_file.append(csv_file_2)\n",
    "            file_df_2 = pd.read_csv(file_path_2)\n",
    "            file_df_2 = file_df_2[to_consider_columns]\n",
    "            file_df_2 = file_df_2.apply(pd.to_numeric, errors = 'coerce')\n",
    "            #file_df_2.fillna(2,inplace=True)\n",
    "            \n",
    "            ks_statistic_produktname, ks_p_value_produktname = ks_2samp(file_df_1['Produktname_dic3'], file_df_2['Produktname_dic3'])\n",
    "            ks_statistic_modell_list, ks_p_value_modell_list = ks_2samp(file_df_1['Modell_Liste_3g'], file_df_2['Modell_Liste_3g'])\n",
    "             \n",
    "            values = [ks_p_value_produktname,ks_p_value_modell_list]\n",
    "            ks_statistc_produkt_name.append(ks_statistic_produktname)\n",
    "            ks_statistic_model_list.append(ks_statistic_modell_list)\n",
    "            \n",
    "            alpha = 0.05/2\n",
    "            \n",
    "            # Check if any value is less than the threshold\n",
    "            any_value_below_threshold = any(value < alpha for value in values)\n",
    "\n",
    "            # Print the result\n",
    "            if any_value_below_threshold:\n",
    "                similiar.append(0)\n",
    "            else:\n",
    "                similiar.append(1)\n",
    "\n",
    "            \n",
    "            \n",
    "df = pd.DataFrame({'first_file': first_file, 'second_file': second_file,'ks_statistc_produkt_name':ks_statistc_produkt_name, 'ks_statistic_model_list':ks_statistic_model_list ,'similiar':similiar})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352335d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_df = result_updated.loc[(result_updated['second_file']=='www.buzzillions.com_cammarkt.com.csv')]\n",
    "second_df = result_updated.loc[(result_updated['second_file']=='www.garricks.com.au_cammarkt.com.csv')]\n",
    "\n",
    "\n",
    "# Extracting the unique values from the 'ID' column in both DataFrames\n",
    "intersection_values = pd.Series(list(set(first_df['first_file']).intersection(second_df['first_file'])))\n",
    "print(len(intersection_values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a6be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = result_updated.loc[(result_updated['first_file']=='www.wexphotographic.com_www.pcconnection.com.csv')]\n",
    "\n",
    "d.sort_values(by='similiar',ascending=False)\n",
    "\n",
    "\n",
    "# www.eglobalcentral.co.uk_www.priceme.co.nz.csv ==> buy.net_www.priceme.co.nz.csv ==> www.gosale.com_www.priceme.co.nz.csv\n",
    "# www.gosale.com_www.eglobalcentral.co.uk.csv \n",
    "\n",
    "\n",
    "#buy.net_cammarkt.com.csv ==> www.garricks.com.au_cammarkt.com.csv ==> www.garricks.com.au_www.pcconnection.com.csv\n",
    " \n",
    "\n",
    "# www.camerafarm.com.au_www.priceme.co.nz.csv ==> buy.net_www.camerafarm.com.au.csv ==> www.wexphotographic.com_www.henrys.com.csv\n",
    "# www.camerafarm.com.au_www.henrys.com.csv \n",
    "\n",
    "# buy.net_www.camerafarm.com.au.csv ==> www.wexphotographic.com_www.henrys.com.csv ==> www.camerafarm.com.au_www.henrys.com.csv\n",
    "#  \n",
    "\n",
    "\n",
    "# \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lead = pd.read_csv(os.path.join(path_to_sim_vector_folder, 'buy.net_cammarkt.com.csv'))\n",
    "\n",
    "# Value to exclude from feature extraction\n",
    "nan_replacement_value = 2 \n",
    "\n",
    "# Columns to extracte from feature extraction \n",
    "to_consider_columns = ['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "                       'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30', 'Breite_NumMaxProz30', \n",
    "                       'HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3']\n",
    "\n",
    "\n",
    "df_lead = df_lead[to_consider_columns]\n",
    "# convert \"/\" into Nan\n",
    "df_lead = df_lead.apply(pd.to_numeric, errors= 'coerce')\n",
    "col = ['Produktname_dic3', 'Modell_Liste_3g']\n",
    "# Replace NaN values in the feature vector with a specific numerical value\n",
    "df_lead.fillna(nan_replacement_value, inplace=True)\n",
    "\n",
    "df_lead = df_lead[col]\n",
    "\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(path_to_sim_vector_folder, 'www.garricks.com.au_cammarkt.com.csv'))\n",
    "\n",
    "# Value to exclude from feature extraction\n",
    "nan_replacement_value = 2 \n",
    "\n",
    "# Columns to extracte from feature extraction \n",
    "to_consider_columns = ['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "                       'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30', 'Breite_NumMaxProz30', \n",
    "                       'HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3']\n",
    "\n",
    "\n",
    "df_test = df_test[to_consider_columns]\n",
    "# convert \"/\" into Nan\n",
    "df_test = df_test.apply(pd.to_numeric, errors= 'coerce')\n",
    "col = ['Produktname_dic3', 'Modell_Liste_3g']\n",
    "# Replace NaN values in the feature vector with a specific numerical value\n",
    "df_test.fillna(nan_replacement_value, inplace=True)\n",
    "\n",
    "df_test = df_test[col]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lead.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f5e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_rep,compared_resources = generate_data_representation_very_new()\n",
    "meta_data_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Meta Data representations generation(There are methods: generate_data_representation() ,generate_data_representation_better_values() and  generate_data_representations_all_values()\n",
    "    meta_data_rep,compared_resources = generate_data_representation_very_new()\n",
    "    # Clustering of Meta Data representations\n",
    "    number_of_clusters, avg_silhouette_score = get_best_number_of_clusters_using_wcss_score(meta_data_rep)\n",
    "    # Predict clusters for each data point\n",
    "    clusters = predict_clusters(meta_data_rep,number_of_clusters,compared_resources)\n",
    "    return clusters\n",
    "    # Modling(Create xgboost models for each cluster)\n",
    "    #generate_models(clusters)\n",
    "    # Predicting\n",
    "    #models_training(clusters)\n",
    "    \n",
    "    \n",
    "    \n",
    "clusters =  main()  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6929e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "# specify the path of the data folder \n",
    "path_to_data_folder = '/Users/abdulnaser/Desktop/Masterarbeit/metadatatransferlearning-main/meta_tl/data/'\n",
    "\n",
    "# Specify the folder path containing the csv files \n",
    "path_to_sim_vector_folder =  path_to_data_folder + 'sim_dataframes/'\n",
    "\n",
    "# Columns to extracte from feature extraction \n",
    "to_consider_columns = ['Produktname_dic3','Modell_Liste_3g']\n",
    "\n",
    "\n",
    "cluster = []\n",
    "first_file = []\n",
    "second_file = []\n",
    "ks_statistc_produkt_name = []\n",
    "ks_statistic_model_list = []\n",
    "similiar = []\n",
    "cluster_count = []\n",
    "\n",
    "for key, value in clusters.items():\n",
    "    sources = value\n",
    "    for csv_file_1 in sources:\n",
    "        file_path_1 = os.path.join(path_to_sim_vector_folder, csv_file_1)\n",
    "        file_df_1 = pd.read_csv(file_path_1)\n",
    "        file_df_1 = file_df_1[to_consider_columns]\n",
    "        file_df_1 = file_df_1.apply(pd.to_numeric, errors = 'coerce')\n",
    "        #file_df_1.fillna(2,inplace=True)\n",
    "    \n",
    "        for csv_file_2 in sources:\n",
    "            file_path_2 = os.path.join(path_to_sim_vector_folder, csv_file_2)\n",
    "            if file_path_1 != file_path_2:\n",
    "                cluster.append(key)\n",
    "                first_file.append(csv_file_1)\n",
    "                second_file.append(csv_file_2)\n",
    "                cluster_count.append(len(value))\n",
    "                file_df_2 = pd.read_csv(file_path_2)\n",
    "                file_df_2 = file_df_2[to_consider_columns]\n",
    "                file_df_2 = file_df_2.apply(pd.to_numeric, errors = 'coerce')\n",
    "                #file_df_2.fillna(2,inplace=True)\n",
    "\n",
    "                ks_statistic_produktname, ks_p_value_produktname = ks_2samp(file_df_1['Produktname_dic3'], file_df_2['Produktname_dic3'])\n",
    "                ks_statistic_modell_list, ks_p_value_modell_list = ks_2samp(file_df_1['Modell_Liste_3g'], file_df_2['Modell_Liste_3g'])\n",
    "\n",
    "                values = [ks_p_value_produktname,ks_p_value_modell_list]\n",
    "                ks_statistc_produkt_name.append(ks_statistic_produktname)\n",
    "                ks_statistic_model_list.append(ks_statistic_modell_list)\n",
    "\n",
    "                alpha = 0.05/2\n",
    "\n",
    "                # Check if any value is less than the threshold\n",
    "                any_value_below_threshold = any(value < alpha for value in values)\n",
    "\n",
    "                # Print the result\n",
    "                if any_value_below_threshold:\n",
    "                    similiar.append(0)\n",
    "                else:\n",
    "                    similiar.append(1)\n",
    "\n",
    "            \n",
    "            \n",
    "df = pd.DataFrame({'cluster':cluster,'first_file': first_file, 'second_file': second_file,'ks_statistc_produkt_name':ks_statistc_produkt_name, 'ks_statistic_model_list':ks_statistic_model_list ,'similiar':similiar,'cluster_count':cluster_count})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23896de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a5a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated = df.groupby(['cluster', 'first_file','cluster_count'])['similiar'].sum().reset_index()\n",
    "df_updated['percentage'] = df_updated['similiar'] / df_updated['cluster_count']\n",
    "idxmax_values = df_updated.groupby(['cluster','first_file'])['percentage'].idxmax()\n",
    "# Use the index to get the corresponding rows\n",
    "result = df_updated.loc[idxmax_values]\n",
    "lengths = []\n",
    "for value in result['first_file']:\n",
    "    file_path_1 = os.path.join(path_to_sim_vector_folder, value)\n",
    "    file_df_1 = pd.read_csv(file_path_1)\n",
    "    lengths.append(file_df_1.shape[0])\n",
    "    \n",
    "result['lengths'] = lengths\n",
    "\n",
    "result.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde558ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxmax_values = result.groupby(['cluster'])['percentage'].idxmax()\n",
    "result_updated = result.loc[idxmax_values]\n",
    "result_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98622ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lead = pd.read_csv(os.path.join(path_to_sim_vector_folder, 'buy.net_www.ilgs.net.csv'))\n",
    "\n",
    "# Value to exclude from feature extraction\n",
    "nan_replacement_value = 2 \n",
    "\n",
    "# Columns to extracte from feature extraction \n",
    "to_consider_columns = ['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "                       'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30', 'Breite_NumMaxProz30', \n",
    "                       'HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3']\n",
    "\n",
    "\n",
    "df_lead = df_lead[to_consider_columns]\n",
    "# convert \"/\" into Nan\n",
    "df_lead = df_lead.apply(pd.to_numeric, errors= 'coerce')\n",
    "col = ['Produktname_dic3', 'Modell_Liste_3g']\n",
    "# Replace NaN values in the feature vector with a specific numerical value\n",
    "df_lead.fillna(nan_replacement_value, inplace=True)\n",
    "\n",
    "df_lead = df_lead[col]\n",
    "\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(path_to_sim_vector_folder, 'www.camerafarm.com.au_cammarkt.com.csv'))\n",
    "\n",
    "# Value to exclude from feature extraction\n",
    "nan_replacement_value = 2 \n",
    "\n",
    "# Columns to extracte from feature extraction \n",
    "to_consider_columns = ['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "                       'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30', 'Breite_NumMaxProz30', \n",
    "                       'HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3']\n",
    "\n",
    "\n",
    "df_test = df_test[to_consider_columns]\n",
    "# convert \"/\" into Nan\n",
    "df_test = df_test.apply(pd.to_numeric, errors= 'coerce')\n",
    "col = ['Produktname_dic3', 'Modell_Liste_3g']\n",
    "# Replace NaN values in the feature vector with a specific numerical value\n",
    "df_test.fillna(nan_replacement_value, inplace=True)\n",
    "\n",
    "df_test = df_test[col]\n",
    "\n",
    "\n",
    "ks_statistic, ks_p_value_produktname = ks_2samp(df_lead['Produktname_dic3'], df_test['Produktname_dic3'])\n",
    "ks_statistic, ks_p_value_modell_list = ks_2samp(df_lead['Modell_Liste_3g'], df_test['Modell_Liste_3g'])\n",
    "print(ks_p_value_produktname)\n",
    "print(ks_p_value_modell_list)\n",
    "values = [ks_p_value_produktname,ks_p_value_modell_list]\n",
    "            \n",
    "alpha = 0.05/2\n",
    "            \n",
    "# Check if any value is less than the threshold\n",
    "any_value_below_threshold = any(value < alpha for value in values)\n",
    "\n",
    "if any_value_below_threshold:\n",
    "    print(\"reject null hypothesis\")\n",
    "\n",
    "else:\n",
    "    print(\"do not reject null hypothesis\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_zero = []\n",
    "files_zero = []\n",
    "performance_zero = []\n",
    "\n",
    "cluster_one = []\n",
    "files_one = []\n",
    "performance_one = []\n",
    "\n",
    "# 2. Build an XGBoost model and train it on the data of the dataframe with the most number of rows \n",
    "cluster_zero_file = pd.read_csv(path_to_sim_vector_folder + 'buy.net_www.ilgs.net.csv')\n",
    "cluster_zero_file.drop(columns=['record_compared_1','record_compared_2','Modell_no_Liste_TruncateBegin20','Unnamed: 0','recId','recId.1'], axis=1, inplace=True)\n",
    "\n",
    "cluster_zero_file = cluster_zero_file[['Produktname_dic3', 'Modell_Liste_3g', 'is_match']]\n",
    "\n",
    "# Replace \"/\" with 9999 in 'is_match' column\n",
    "cluster_zero_file.replace('/', 2, inplace=True)\n",
    "\n",
    "# Convert all columns to numerical data types \n",
    "cluster_zero_file = cluster_zero_file.apply(pd.to_numeric, errors='coerce')\n",
    "# Assuming the last column is the target variable and the rest are features\n",
    "X = cluster_zero_file.iloc[:, :-1] # Features (all columns except the last one)\n",
    "y = cluster_zero_file.iloc[: , -1] # Taregt variable (is_match)\n",
    "\n",
    "# Create an XGBoost classifier \n",
    "model_zero_cluter = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Train the model in the taining data\n",
    "model_zero_cluter.fit(X, y)\n",
    "\n",
    "files_to_predict_by_cluster_zero = df.loc[df['first_file']== 'www.price-hunt.com_cammarkt.com.csv']\n",
    "\n",
    "for file in files_to_predict_by_cluster_zero['second_file']:\n",
    "    cluster_zero.append(0)\n",
    "    files_zero.append(file)\n",
    "    file_data = pd.read_csv(path_to_sim_vector_folder + file)\n",
    "    file_data.drop(columns=['record_compared_1','record_compared_2','Modell_no_Liste_TruncateBegin20','Unnamed: 0','recId','recId.1'], axis=1, inplace=True)\n",
    "\n",
    "    file_data = file_data[['Produktname_dic3', 'Modell_Liste_3g', 'is_match']]\n",
    "\n",
    "    # Replace \"/\" with 9999 in 'is_match' column\n",
    "    file_data.replace('/', 2, inplace=True)\n",
    "\n",
    "    # Convert all columns to numerical data types \n",
    "    file_data = file_data.apply(pd.to_numeric, errors='coerce')\n",
    "    # Assuming the last column is the target variable and the rest are features\n",
    "    X_test = file_data.iloc[:, :-1] # Features (all columns except the last one)\n",
    "    y_test = file_data.iloc[: , -1] # Taregt variable (is_match)\n",
    "    \n",
    "    predictions = model_zero_cluter.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    performance_zero.append(accuracy)\n",
    "   \n",
    "\n",
    "\n",
    "df_cluster_zero = pd.DataFrame({\n",
    "        'cluster': cluster_zero,\n",
    "        'files': files_zero,\n",
    "        'performance': performance_zero\n",
    "     })\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# 2. Build an XGBoost model and train it on the data of the dataframe with the most number of rows \n",
    "cluster_one_file = pd.read_csv(path_to_sim_vector_folder + 'buy.net_www.ilgs.net.csv')\n",
    "cluster_one_file.drop(columns=['record_compared_1','record_compared_2','Modell_no_Liste_TruncateBegin20','Unnamed: 0','recId','recId.1'], axis=1, inplace=True)\n",
    "\n",
    "cluster_one_file = cluster_one_file[['Produktname_dic3', 'Modell_Liste_3g', 'is_match']]\n",
    "\n",
    "# Replace \"/\" with 9999 in 'is_match' column\n",
    "cluster_one_file.replace('/', 2, inplace=True)\n",
    "\n",
    "# Convert all columns to numerical data types \n",
    "cluster_one_file = cluster_one_file.apply(pd.to_numeric, errors='coerce')\n",
    "# Assuming the last column is the target variable and the rest are features\n",
    "X = cluster_one_file.iloc[:, :-1] # Features (all columns except the last one)\n",
    "y = cluster_one_file.iloc[: , -1] # Taregt variable (is_match)\n",
    "\n",
    "# Create an XGBoost classifier \n",
    "model_one_cluter = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Train the model in the taining data\n",
    "model_one_cluter.fit(X, y)\n",
    "\n",
    "files_to_predict_by_cluster_one = df.loc[df['first_file']== 'www.camerafarm.com.au_cammarkt.com.csv']\n",
    "\n",
    "for file in files_to_predict_by_cluster_one['second_file']:\n",
    "    cluster_one.append(0)\n",
    "    files_one.append(file)\n",
    "    file_data = pd.read_csv(path_to_sim_vector_folder + file)\n",
    "    file_data.drop(columns=['record_compared_1','record_compared_2','Modell_no_Liste_TruncateBegin20','Unnamed: 0','recId','recId.1'], axis=1, inplace=True)\n",
    "\n",
    "    file_data = file_data[['Produktname_dic3', 'Modell_Liste_3g', 'is_match']]\n",
    "\n",
    "    # Replace \"/\" with 9999 in 'is_match' column\n",
    "    file_data.replace('/', 2, inplace=True)\n",
    "\n",
    "    # Convert all columns to numerical data types \n",
    "    file_data = file_data.apply(pd.to_numeric, errors='coerce')\n",
    "    # Assuming the last column is the target variable and the rest are features\n",
    "    X_test = file_data.iloc[:, :-1] # Features (all columns except the last one)\n",
    "    y_test = file_data.iloc[: , -1] # Taregt variable (is_match)\n",
    "    \n",
    "    predictions = model_one_cluter.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    performance_one.append(accuracy)\n",
    "   \n",
    "\n",
    "\n",
    "df_cluster_one = pd.DataFrame({\n",
    "        'cluster': cluster_one,\n",
    "        'files': files_one,\n",
    "        'performance': performance_one\n",
    "     })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb653f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_one['performance'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the path of the data folder \n",
    "path_to_data_folder = '/Users/abdulnaser/Desktop/Masterarbeit/metadatatransferlearning-main/meta_tl/data/'\n",
    "\n",
    "# Specify the folder path containing the csv files \n",
    "path_to_sim_vector_folder =  path_to_data_folder + 'sim_dataframes/'\n",
    "\n",
    "# Get a list of all CSV files in the folder \n",
    "csv_files = [file for file in os.listdir(path_to_sim_vector_folder) if file.endswith('.csv')]\n",
    "\n",
    "# Columns to extracte from feature extraction \n",
    "to_consider_columns = ['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "                       'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30', 'Breite_NumMaxProz30', \n",
    "                       'HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3']\n",
    "\n",
    "\n",
    "first_file = []\n",
    "second_file = []\n",
    "ks_statistc_produkt_name = []\n",
    "ks_statistic_model_list = []\n",
    "similiar = []\n",
    "\n",
    "for csv_file_1 in csv_files:\n",
    "    print(f\"First dataset is {csv_file_1} \")\n",
    "    file_path_1 = os.path.join(path_to_sim_vector_folder, csv_file_1)\n",
    "    file_df_1 = pd.read_csv(file_path_1)\n",
    "    file_df_1 = file_df_1[to_consider_columns]\n",
    "    file_df_1 = file_df_1.apply(pd.to_numeric, errors = 'coerce')\n",
    "    file_df_1.fillna(2,inplace=True)\n",
    "    \n",
    "    for csv_file_2 in csv_files:\n",
    "        file_path_2 = os.path.join(path_to_sim_vector_folder, csv_file_2)\n",
    "        if file_path_1 != file_path_2:\n",
    "            print(f\"second dataset is {csv_file_2}\")\n",
    "            first_file.append(csv_file_1)\n",
    "            second_file.append(csv_file_2)\n",
    "            file_df_2 = pd.read_csv(file_path_2)\n",
    "            file_df_2 = file_df_2[to_consider_columns]\n",
    "            file_df_2 = file_df_2.apply(pd.to_numeric, errors = 'coerce')\n",
    "            file_df_2.fillna(2,inplace=True)\n",
    "            \n",
    "            ks_statistic_produktname, ks_p_value_produktname = ks_2samp(file_df_1['Produktname_dic3'], file_df_2['Produktname_dic3'])\n",
    "            ks_statistic_modell_list, ks_p_value_modell_list = ks_2samp(file_df_1['Modell_Liste_3g'], file_df_2['Modell_Liste_3g'])\n",
    "                        \n",
    "                        \n",
    "            ks_statistic_MPN_Liste_TruncateBegin20, ks_p_value_MPN_Liste_TruncateBegin20 = ks_2samp(file_df_1['MPN_Liste_TruncateBegin20'], file_df_2['MPN_Liste_TruncateBegin20'])\n",
    "            ks_statistic_EAN_Liste_TruncateBegin20, ks_p_value_EAN_Liste_TruncateBegin20 = ks_2samp(file_df_1['EAN_Liste_TruncateBegin20'], file_df_2['EAN_Liste_TruncateBegin20'])\n",
    "\n",
    "            \n",
    "            ks_statistic_Digital_zoom_NumMaxProz30, ks_p_value_Digital_zoom_NumMaxProz30 = ks_2samp(file_df_1['Digital_zoom_NumMaxProz30'], file_df_2['Digital_zoom_NumMaxProz30'])\n",
    "            ks_statistic_optischer_zoom_NumMaxProz30, ks_p_value_optischer_zoom_NumMaxProz30 = ks_2samp(file_df_1['optischer_zoom_NumMaxProz30'], file_df_2['optischer_zoom_NumMaxProz30'])\n",
    "            \n",
    "                        \n",
    "            ks_statistic_HÃ¶he_NumMaxProz30, ks_p_value_HÃ¶he_NumMaxProz30 = ks_2samp(file_df_1['HÃ¶he_NumMaxProz30'], file_df_2['HÃ¶he_NumMaxProz30'])\n",
    "            ks_statistic_Breite_NumMaxProz30, ks_p_value_Breite_NumMaxProz30 = ks_2samp(file_df_1['Breite_NumMaxProz30'], file_df_2['Breite_NumMaxProz30'])\n",
    "              \n",
    "            ks_statistic_Gewicht_NumMaxProz30, ks_p_value_Gewicht_NumMaxProz30 = ks_2samp(file_df_1['Gewicht_NumMaxProz30'], file_df_2['Gewicht_NumMaxProz30'])\n",
    "            ks_statistic_Sensortyp_Jaccard3, ks_p_value_Sensortyp_Jaccard3 = ks_2samp(file_df_1['Sensortyp_Jaccard3'], file_df_2['Sensortyp_Jaccard3'])\n",
    "              \n",
    "            \n",
    "                \n",
    "            values = [ks_p_value_produktname,ks_p_value_modell_list,ks_p_value_MPN_Liste_TruncateBegin20,ks_p_value_EAN_Liste_TruncateBegin20,\n",
    "                      ks_p_value_Digital_zoom_NumMaxProz30,ks_p_value_optischer_zoom_NumMaxProz30,ks_p_value_HÃ¶he_NumMaxProz30,ks_p_value_Breite_NumMaxProz30,\n",
    "                      ks_p_value_Gewicht_NumMaxProz30,ks_p_value_Sensortyp_Jaccard3]\n",
    "            ks_statistc_produkt_name.append(ks_statistic_produktname)\n",
    "            ks_statistic_model_list.append(ks_statistic_modell_list)\n",
    "            \n",
    "            alpha = 0.05/10\n",
    "            \n",
    "            # Check if any value is less than the threshold\n",
    "            any_value_below_threshold = any(value < alpha for value in values)\n",
    "\n",
    "            # Print the result\n",
    "            if any_value_below_threshold:\n",
    "                similiar.append(0)\n",
    "            else:\n",
    "                similiar.append(1)\n",
    "\n",
    "            \n",
    "            \n",
    "df = pd.DataFrame({'first_file': first_file, 'second_file': second_file,'ks_statistc_produkt_name':ks_statistc_produkt_name, 'ks_statistic_model_list':ks_statistic_model_list ,'similiar':similiar})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0fa834",
   "metadata": {},
   "outputs": [],
   "source": [
    "d, d_1 = generate_data_representation_very_new_test()\n",
    "d.to_csv('test_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b84152",
   "metadata": {},
   "source": [
    "# TEST VERY NEW "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c4c7a2",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7955d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import community\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp\n",
    "from networkx.algorithms.community import asyn_lpa_communities\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2b1cf",
   "metadata": {},
   "source": [
    "# Set Folder Pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1046898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the path of the data folder \n",
    "path_to_data_folder = '/Users/abdulnaser/Desktop/Masterarbeit/metadatatransferlearning-main/meta_tl/data/'\n",
    "\n",
    "# Specify the folder path containing the csv files \n",
    "path_to_sim_vector_folder =  path_to_data_folder + 'sim_dataframes/'\n",
    "# www.ebay.com_www.pcconnection.com.csv    www.camerafarm.com.au_cammarkt.com.csv \t\n",
    "www.ebay.com_www.pcconnection.com.csv      www.camerafarm.com.au_www.priceme.co.nz.csv \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11554afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_1 = pd.read_csv(path_to_sim_vector_folder + 'www.ebay.com_www.pcconnection.com.csv')\n",
    "temp_data_2 = pd.read_csv(path_to_sim_vector_folder + 'www.camerafarm.com.au_cammarkt.com.csv')\n",
    "\n",
    "# Replace 'column_name' with the actual column you want to plot\n",
    "ecdf_1 = sm.distributions.ECDF(temp_data_1['Produktname_dic3'])\n",
    "ecdf_2 = sm.distributions.ECDF(temp_data_2['Produktname_dic3'])\n",
    "\n",
    "# Interpolate CDFs to have the same set of points\n",
    "x_values = np.unique(np.concatenate([ecdf_1.x, ecdf_2.x]))\n",
    "y_values_1 = np.interp(x_values, ecdf_1.x, ecdf_1.y, left=0, right=1)\n",
    "y_values_2 = np.interp(x_values, ecdf_2.x, ecdf_2.y, left=0, right=1)\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(10, 6))  # Adjust the width and height as needed\n",
    "\n",
    "\n",
    "# Plot the interpolated CDFs\n",
    "plt.plot(x_values, y_values_1, label='Produktname_dic3_ebay_pcconnection')\n",
    "plt.plot(x_values, y_values_2, label='Produktname_dic3_camerafarm_cammarkt')\n",
    "\n",
    "# Calculate the maximum absolute difference\n",
    "max_abs_diff = np.max(np.abs(y_values_1 - y_values_2))\n",
    "print(max_abs_diff)\n",
    "\n",
    "# Find the index of the maximum absolute difference\n",
    "max_diff_index = np.argmax(np.abs(y_values_1 - y_values_2))\n",
    "\n",
    "# Plot a line at the maximum difference point\n",
    "plt.axvline(x=x_values[max_diff_index], color='red', linestyle='--', label='Max Difference')\n",
    "\n",
    "plt.xlabel('Similarity Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Interpolated Cumulative Distribution Function (CDF) with Max Difference Line')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220540f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_1 = pd.read_csv(path_to_sim_vector_folder + 'www.ebay.com_www.pcconnection.com.csv')\n",
    "temp_data_2 = pd.read_csv(path_to_sim_vector_folder + 'www.eglobalcentral.co.uk_www.priceme.co.nz.csv')\n",
    "\n",
    "# Replace 'column_name' with the actual column you want to plot\n",
    "ecdf_1 = sm.distributions.ECDF(temp_data_1['Produktname_dic3'])\n",
    "ecdf_2 = sm.distributions.ECDF(temp_data_2['Produktname_dic3'])\n",
    "\n",
    "# Interpolate CDFs to have the same set of points\n",
    "x_values = np.unique(np.concatenate([ecdf_1.x, ecdf_2.x]))\n",
    "y_values_1 = np.interp(x_values, ecdf_1.x, ecdf_1.y, left=0, right=1)\n",
    "y_values_2 = np.interp(x_values, ecdf_2.x, ecdf_2.y, left=0, right=1)\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(10, 6))  # Adjust the width and height as needed\n",
    "# Plot the interpolated CDFs\n",
    "plt.plot(x_values, y_values_1, label='Produktname_dic3_ebay_pcconnection')\n",
    "plt.plot(x_values, y_values_2, label='Produktname_dic3_eglobalcentral_priceme')\n",
    "\n",
    "# Calculate the maximum absolute difference\n",
    "max_abs_diff = np.max(np.abs(y_values_1 - y_values_2))\n",
    "print(max_abs_diff)\n",
    "\n",
    "# Find the index of the maximum absolute difference\n",
    "max_diff_index = np.argmax(np.abs(y_values_1 - y_values_2))\n",
    "\n",
    "# Plot a line at the maximum difference point\n",
    "plt.axvline(x=x_values[max_diff_index], color='red', linestyle='--', label='Max Difference')\n",
    "\n",
    "plt.xlabel('Similarity Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Interpolated Cumulative Distribution Function (CDF) with Max Difference Line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_1 = pd.read_csv(path_to_sim_vector_folder + 'www.ebay.com_www.pcconnection.com.csv')\n",
    "temp_data_2 = pd.read_csv(path_to_sim_vector_folder + 'www.eglobalcentral.co.uk_www.priceme.co.nz.csv')\n",
    "\n",
    "# Replace 'column_name' with the actual column you want to plot\n",
    "ecdf_1 = sm.distributions.ECDF(temp_data_1['Produktname_dic3'])\n",
    "ecdf_2 = sm.distributions.ECDF(temp_data_2['Produktname_dic3'])\n",
    "\n",
    "# Interpolate CDFs to have the same set of points\n",
    "x_values = np.unique(np.concatenate([ecdf_1.x, ecdf_2.x]))\n",
    "y_values_1 = np.interp(x_values, ecdf_1.x, ecdf_1.y, left=0, right=1)\n",
    "y_values_2 = np.interp(x_values, ecdf_2.x, ecdf_2.y, left=0, right=1)\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(10, 6))  # Adjust the width and height as needed\n",
    "# Plot the interpolated CDFs\n",
    "plt.plot(x_values, y_values_1, label='Produktname_dic3_ebay_pcconnection')\n",
    "plt.plot(x_values, y_values_2, label='Produktname_dic3_eglobalcentral_priceme')\n",
    "\n",
    "# Calculate the maximum absolute difference\n",
    "max_abs_diff = np.max(np.abs(y_values_1 - y_values_2))\n",
    "print(max_abs_diff)\n",
    "\n",
    "# Find the index of the maximum absolute difference\n",
    "max_diff_index = np.argmax(np.abs(y_values_1 - y_values_2))\n",
    "\n",
    "# Plot a line at the maximum difference point\n",
    "plt.axvline(x=x_values[max_diff_index], color='red', linestyle='--', label='Max Difference')\n",
    "\n",
    "plt.xlabel('Similarity Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Interpolated Cumulative Distribution Function (CDF) with Max Difference Line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486765ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_1 = pd.read_csv(path_to_sim_vector_folder + 'www.ebay.com_www.pcconnection.com.csv')\n",
    "temp_data_2 = pd.read_csv(path_to_sim_vector_folder + 'www.garricks.com.au_www.ebay.com.csv')\n",
    "\n",
    "# Replace 'column_name' with the actual column you want to plot\n",
    "ecdf_1 = sm.distributions.ECDF(temp_data_1['Produktname_dic3'])\n",
    "ecdf_2 = sm.distributions.ECDF(temp_data_2['Produktname_dic3'])\n",
    "\n",
    "# Interpolate CDFs to have the same set of points\n",
    "x_values = np.unique(np.concatenate([ecdf_1.x, ecdf_2.x]))\n",
    "y_values_1 = np.interp(x_values, ecdf_1.x, ecdf_1.y, left=0, right=1)\n",
    "y_values_2 = np.interp(x_values, ecdf_2.x, ecdf_2.y, left=0, right=1)\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(10, 6))  # Adjust the width and height as needed\n",
    "# Plot the interpolated CDFs\n",
    "plt.plot(x_values, y_values_1, label='Produktname_dic3_ebay_pcconnection')\n",
    "plt.plot(x_values, y_values_2, label='Produktname_dic3_eglobalcentral_priceme')\n",
    "\n",
    "# Calculate the maximum absolute difference\n",
    "max_abs_diff = np.max(np.abs(y_values_1 - y_values_2))\n",
    "print(max_abs_diff)\n",
    "\n",
    "# Find the index of the maximum absolute difference\n",
    "max_diff_index = np.argmax(np.abs(y_values_1 - y_values_2))\n",
    "\n",
    "# Plot a line at the maximum difference point\n",
    "plt.axvline(x=x_values[max_diff_index], color='red', linestyle='--', label='Max Difference')\n",
    "\n",
    "plt.xlabel('Similarity Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Interpolated Cumulative Distribution Function (CDF) with Max Difference Line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read and prepare the dataframes\n",
    "temp_data_1 = prepare_dataframe_to_similarity_comparison(path_to_sim_vector_folder + 'www.ebay.com_www.pcconnection.com.csv')\n",
    "temp_data_2 = prepare_dataframe_to_similarity_comparison(path_to_sim_vector_folder + 'www.garricks.com.au_www.ebay.com.csv')\n",
    "\n",
    "# Drop rows with NaN values in the specified columns\n",
    "temp_data_1 = temp_data_1.dropna(subset=numeric_columns)\n",
    "temp_data_2 = temp_data_2.dropna(subset=numeric_columns)\n",
    "\n",
    "# Replace 'column_name' with the actual column you want to plot\n",
    "ecdf_1 = sm.distributions.ECDF(temp_data_1['Modell_Liste_3g'])\n",
    "ecdf_2 = sm.distributions.ECDF(temp_data_2['Modell_Liste_3g'])\n",
    "\n",
    "# Interpolate CDFs to have the same set of points\n",
    "x_values = np.unique(np.concatenate([ecdf_1.x, ecdf_2.x]))\n",
    "y_values_1 = np.interp(x_values, ecdf_1.x, ecdf_1.y, left=0, right=1)\n",
    "y_values_2 = np.interp(x_values, ecdf_2.x, ecdf_2.y, left=0, right=1)\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(10, 6))  # Adjust the width and height as needed\n",
    "# Plot the interpolated CDFs\n",
    "plt.plot(x_values, y_values_1, label='Modell_Liste_3g_ebay_pcconnection')\n",
    "plt.plot(x_values, y_values_2, label='Modell_Liste_3g_eglobalcentral_priceme')\n",
    "\n",
    "# Calculate the maximum absolute difference\n",
    "max_abs_diff = np.max(np.abs(y_values_1 - y_values_2))\n",
    "print(max_abs_diff)\n",
    "\n",
    "# Find the index of the maximum absolute difference\n",
    "max_diff_index = np.argmax(np.abs(y_values_1 - y_values_2))\n",
    "\n",
    "# Plot a line at the maximum difference point\n",
    "plt.axvline(x=x_values[max_diff_index], color='red', linestyle='--', label='Max Difference')\n",
    "\n",
    "plt.xlabel('Similarity Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Interpolated Cumulative Distribution Function (CDF) with Max Difference Line')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d90d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_1 = pd.read_csv(path_to_sim_vector_folder + 'www.garricks.com.au_www.ebay.com.csv')\n",
    "temp_data_2 = pd.read_csv(path_to_sim_vector_folder + 'www.ebay.com_www.eglobalcentral.co.uk.csv')\n",
    "\n",
    "# Replace 'column_name' with the actual column you want to plot\n",
    "ecdf_1 = sm.distributions.ECDF(temp_data_1['Produktname_dic3'])\n",
    "ecdf_2 = sm.distributions.ECDF(temp_data_2['Produktname_dic3'])\n",
    "\n",
    "# Interpolate CDFs to have the same set of points\n",
    "x_values = np.unique(np.concatenate([ecdf_1.x, ecdf_2.x]))\n",
    "y_values_1 = np.interp(x_values, ecdf_1.x, ecdf_1.y, left=0, right=1)\n",
    "y_values_2 = np.interp(x_values, ecdf_2.x, ecdf_2.y, left=0, right=1)\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(10, 6))  # Adjust the width and height as needed\n",
    "# Plot the interpolated CDFs\n",
    "plt.plot(x_values, y_values_1, label='Produktname_dic3_ebay_pcconnection')\n",
    "plt.plot(x_values, y_values_2, label='Produktname_dic3_eglobalcentral_priceme')\n",
    "\n",
    "# Calculate the maximum absolute difference\n",
    "max_abs_diff = np.max(np.abs(y_values_1 - y_values_2))\n",
    "print(max_abs_diff)\n",
    "\n",
    "# Find the index of the maximum absolute difference\n",
    "max_diff_index = np.argmax(np.abs(y_values_1 - y_values_2))\n",
    "\n",
    "# Plot a line at the maximum difference point\n",
    "plt.axvline(x=x_values[max_diff_index], color='red', linestyle='--', label='Max Difference')\n",
    "\n",
    "plt.xlabel('Similarity Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Interpolated Cumulative Distribution Function (CDF) with Max Difference Line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c4660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read and prepare the dataframes\n",
    "temp_data_1 = prepare_dataframe_to_similarity_comparison(path_to_sim_vector_folder + 'www.garricks.com.au_www.ebay.com.csv')\n",
    "temp_data_2 = prepare_dataframe_to_similarity_comparison(path_to_sim_vector_folder + 'www.ebay.com_www.eglobalcentral.co.uk.csv')\n",
    "\n",
    "# Drop rows with NaN values in the specified columns\n",
    "temp_data_1 = temp_data_1.dropna(subset=numeric_columns)\n",
    "temp_data_2 = temp_data_2.dropna(subset=numeric_columns)\n",
    "\n",
    "# Replace 'column_name' with the actual column you want to plot\n",
    "ecdf_1 = sm.distributions.ECDF(temp_data_1['Modell_Liste_3g'])\n",
    "ecdf_2 = sm.distributions.ECDF(temp_data_2['Modell_Liste_3g'])\n",
    "\n",
    "# Interpolate CDFs to have the same set of points\n",
    "x_values = np.unique(np.concatenate([ecdf_1.x, ecdf_2.x]))\n",
    "y_values_1 = np.interp(x_values, ecdf_1.x, ecdf_1.y, left=0, right=1)\n",
    "y_values_2 = np.interp(x_values, ecdf_2.x, ecdf_2.y, left=0, right=1)\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(10, 6))  # Adjust the width and height as needed\n",
    "# Plot the interpolated CDFs\n",
    "plt.plot(x_values, y_values_1, label='Modell_Liste_3g_ebay_pcconnection')\n",
    "plt.plot(x_values, y_values_2, label='Modell_Liste_3g_eglobalcentral_priceme')\n",
    "\n",
    "# Calculate the maximum absolute difference\n",
    "max_abs_diff = np.max(np.abs(y_values_1 - y_values_2))\n",
    "print(max_abs_diff)\n",
    "\n",
    "# Find the index of the maximum absolute difference\n",
    "max_diff_index = np.argmax(np.abs(y_values_1 - y_values_2))\n",
    "\n",
    "# Plot a line at the maximum difference point\n",
    "plt.axvline(x=x_values[max_diff_index], color='red', linestyle='--', label='Max Difference')\n",
    "\n",
    "plt.xlabel('Similarity Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Interpolated Cumulative Distribution Function (CDF) with Max Difference Line')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd67eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_1 = pd.read_csv(path_to_sim_vector_folder + 'www.ebay.com_www.pcconnection.com.csv')\n",
    "temp_data_2 = pd.read_csv(path_to_sim_vector_folder + 'www.ebay.com_www.eglobalcentral.co.uk.csv')\n",
    "\n",
    "# Replace 'column_name' with the actual column you want to plot\n",
    "ecdf_1 = sm.distributions.ECDF(temp_data_1['Produktname_dic3'])\n",
    "ecdf_2 = sm.distributions.ECDF(temp_data_2['Produktname_dic3'])\n",
    "\n",
    "# Interpolate CDFs to have the same set of points\n",
    "x_values = np.unique(np.concatenate([ecdf_1.x, ecdf_2.x]))\n",
    "y_values_1 = np.interp(x_values, ecdf_1.x, ecdf_1.y, left=0, right=1)\n",
    "y_values_2 = np.interp(x_values, ecdf_2.x, ecdf_2.y, left=0, right=1)\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(10, 6))  # Adjust the width and height as needed\n",
    "# Plot the interpolated CDFs\n",
    "plt.plot(x_values, y_values_1, label='Produktname_dic3_ebay_pcconnection')\n",
    "plt.plot(x_values, y_values_2, label='Produktname_dic3_eglobalcentral_priceme')\n",
    "\n",
    "# Calculate the maximum absolute difference\n",
    "max_abs_diff = np.max(np.abs(y_values_1 - y_values_2))\n",
    "print(max_abs_diff)\n",
    "\n",
    "# Find the index of the maximum absolute difference\n",
    "max_diff_index = np.argmax(np.abs(y_values_1 - y_values_2))\n",
    "\n",
    "# Plot a line at the maximum difference point\n",
    "plt.axvline(x=x_values[max_diff_index], color='red', linestyle='--', label='Max Difference')\n",
    "\n",
    "plt.xlabel('Similarity Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Interpolated Cumulative Distribution Function (CDF) with Max Difference Line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_2samp(temp_data_1['Produktname_dic3'], temp_data_2['Produktname_dic3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85934dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_2['Modell_Liste_3g'].dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33840d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_2['Modell_Liste_3g'].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103502b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65955848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b6268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d81a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4608291",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_files_rows_nums = []\n",
    "second_files_rows_nums = []\n",
    "\n",
    "for index, row in similarity_df_second_community.iterrows():\n",
    "    first_file_val = row['first_file']\n",
    "    second_file_val = row['second_file']\n",
    "    \n",
    "    first_file_path = os.path.join(path_to_sim_vector_folder, first_file_val)\n",
    "    second_file_path = os.path.join(path_to_sim_vector_folder, second_file_val)\n",
    "    \n",
    "    first_file_df = pd.read_csv(first_file_path)\n",
    "    second_file_df = pd.read_csv(second_file_path)\n",
    "    \n",
    "    first_files_rows_nums.append(first_file_df.shape[0])\n",
    "    second_files_rows_nums.append(second_file_df.shape[0])\n",
    "    \n",
    "similarity_df_second_community['first_file_rows_num'] = first_files_rows_nums \n",
    "similarity_df_second_community['second_file_rows_num'] = second_files_rows_nums\n",
    "\n",
    "similarity_df_second_community.head()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6438da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each file specify the file that is similiar to it and has the best \n",
    "# average mean with all other files.\n",
    "file_belong_to = {}\n",
    "unique_first_file_values = similarity_df_second_community['first_file'].unique()\n",
    "for file in unique_first_file_values:\n",
    "    temp_second_file_df = similarity_df_second_community.loc[similarity_df_second_community['first_file']==file][['second_file','first_file_rows_num']]\n",
    "    for index, row in temp_second_file_df.iterrows():\n",
    "         if row['second_file'] not in file_belong_to:\n",
    "                file_belong_to[row['second_file']] = [file,row['first_file_rows_num']]\n",
    "         else:\n",
    "                val_2 = file_belong_to[row['second_file']][1]\n",
    "                if row['first_file_rows_num'] > val_2:\n",
    "                    file_belong_to[row['second_file']] = [file,row['first_file_rows_num']]\n",
    "\n",
    "                    \n",
    "# Extract the best files and their count                    \n",
    "files = []\n",
    "belong_to = []  \n",
    "rows_num = []\n",
    "for key, value in file_belong_to.items(): \n",
    "    files.append(value[0])\n",
    "    belong_to.append(key)\n",
    "    rows_num.append(value[1])\n",
    "    \n",
    "    \n",
    "# Create a DataFrame from the two lists\n",
    "cluster_files_df = pd.DataFrame({'first_file': files, 'belong_to': belong_to, 'rows_num':rows_num})\n",
    "cluster_files_df_grouped = cluster_files_df.groupby(['first_file', 'rows_num']).count().reset_index()\n",
    "cluster_files_df_grouped = cluster_files_df_grouped.sort_values(by='belong_to', ascending=False)\n",
    "cluster_files_df_grouped\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the two lists\n",
    "cluster_files_df = pd.DataFrame({'first_file': files, 'belong_to': belong_to, 'rows_num':rows_num})\n",
    "cluster_files_df_grouped = cluster_files_df.groupby(['first_file', 'rows_num']).count().reset_index()\n",
    "cluster_files_df_grouped = cluster_files_df_grouped.sort_values(by='belong_to', ascending=False)\n",
    "cluster_files_df_grouped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_consider = ['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "                       'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30', \n",
    "                       'Breite_NumMaxProz30','HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', \n",
    "                       'Sensortyp_Jaccard3','is_match']\n",
    "\n",
    "overall_accuracy = []\n",
    "\n",
    "for index, row in cluster_files_df_grouped.iterrows():\n",
    "     to_process_file = row['first_file']\n",
    "     print(f\"To process training file is {to_process_file}\")\n",
    "     coressponding_to_process_files = cluster_files_df.loc[cluster_files_df['first_file'] == to_process_file]\n",
    "     to_process_files_sorted = coressponding_to_process_files.sort_values(by='rows_num', ascending=False)['belong_to']\n",
    "        \n",
    "\n",
    "     threshold = 0.94\n",
    "     counter = 1\n",
    "    \n",
    "     # Train the model on the most general dataset 'www.camerafarm.com.au_cammarkt.com.csv'\n",
    "     cluster_file = pd.read_csv(path_to_sim_vector_folder + to_process_file)\n",
    "     cluster_df = prepare_dataframe(cluster_file)\n",
    "        \n",
    "     X = cluster_df.iloc[:, :-1] \n",
    "     y = cluster_df.iloc[: , -1] \n",
    "     model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "     model.fit(X, y)\n",
    "\n",
    "     # Iterate over the files in the Cluster\n",
    "     for file in to_process_files_sorted:\n",
    "         # Prepare the data of the file\n",
    "         file_data = pd.read_csv(path_to_sim_vector_folder + file)\n",
    "         file_df = prepare_dataframe(file_data)\n",
    "         X = file_df.iloc[:, :-1] # Features (all columns except the last one)\n",
    "         y = file_df.iloc[: , -1] # Taregt variable (is_match)   \n",
    "\n",
    "         # prediction\n",
    "         predictions = model.predict(X)\n",
    "         class_probs = model.predict_proba(X)\n",
    "         file_df['pred'] = predictions\n",
    "         file_df[['probabilties_0', 'probabilties_1']] = class_probs\n",
    "\n",
    "\n",
    "         # Calculate the absoulte difference between 'probabilties_0' and 'probabilties_1'\n",
    "         file_df['AbsDiff'] = abs(file_df['probabilties_0'] - file_df['probabilties_1'])\n",
    "         file_df['signifikant_diff'] = (file_df['AbsDiff'] > threshold).astype(int)\n",
    "         strong_preds = file_df.loc[file_df['signifikant_diff']==1].shape[0]\n",
    "         weak_preds = file_df.loc[file_df['signifikant_diff']==0].shape[0]\n",
    "         print(f\"The ratio of weak to strong preds is {weak_preds/strong_preds}\")\n",
    "        \n",
    "        \n",
    "         # Calculate the F1-Score\n",
    "         F1 = f1_score(file_df['is_match'], file_df['pred'])\n",
    "         print(f\"The Accuracy before active learning is {F1}\")\n",
    "\n",
    "         file_df.loc[file_df['signifikant_diff']==0, 'pred'] = file_df['is_match']\n",
    "        \n",
    "         F1 = f1_score(file_df['is_match'], file_df['pred'])\n",
    "         print(f\"The Accuracy after active learning is {F1}\")\n",
    "         overall_accuracy.append(F1)  \n",
    "     \n",
    "     \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(overall_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bebb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_similarity(values_list):\n",
    "    \n",
    "    weights = [30, 30, 10, 10, 2, 2, 2, 2, 2, 2]\n",
    "    \n",
    "    new_values = [1 - x for x in values_list]\n",
    "    \n",
    "    indices_of_3 = [index for index, value in enumerate(new_values) if value == 3]\n",
    "    \n",
    "    new_values_filtered = [value for index, value in enumerate(new_values) if index not in indices_of_3]\n",
    "    weights_filtered = [value for index, value in enumerate(weights) if index not in indices_of_3]\n",
    "    \n",
    "    weighted_mean_similarity = np.average(new_values_filtered, weights=weights_filtered)\n",
    "    \n",
    "    return weighted_mean_similarity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9cc7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_to_similarity_comparison(file_path):\n",
    "    \n",
    "    # Set a threshold for the percentage of NaN values\n",
    "    threshold_percentage = 70\n",
    "    \n",
    "    # Columns to extracte from feature extraction \n",
    "    to_consider_columns = ['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "                           'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30', 'Breite_NumMaxProz30', \n",
    "                           'HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3']\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[to_consider_columns]\n",
    "    # convert \"/\" into Nan\n",
    "    df = df.apply(pd.to_numeric, errors= 'coerce')\n",
    "    # Filter columns where the percentage of NaN values is over the threshold\n",
    "    filtered_columns = df.columns[df.isna().mean() < threshold_percentage / 100]\n",
    "    df = df[filtered_columns]\n",
    "    \n",
    "    return df\n",
    "    \n",
    "     \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(df):\n",
    "    df.drop(columns=['record_compared_1','record_compared_2','Modell_no_Liste_TruncateBegin20'], axis=1, inplace=True)\n",
    "    \n",
    "    df = df[['MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "             'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30', 'Breite_NumMaxProz30', \n",
    "             'HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3', 'is_match']]\n",
    "\n",
    "    # Replace \"/\" with 9999 in 'is_match' column\n",
    "    df.replace('/', 2, inplace=True)\n",
    "    df.replace('/', np.nan, inplace=True)\n",
    "\n",
    "    # Convert all columns to numerical data types \n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a02c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3287f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f5ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qatarNER",
   "language": "python",
   "name": "qatarner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
